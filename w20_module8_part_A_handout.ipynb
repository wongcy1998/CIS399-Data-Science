{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"w20_module8_part_A_handout.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fqYN2skxBOcs"},"source":["<center><h1>Module 8 - Word vectors and word embeddings</h1></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zzuOzBrABOcu"},"source":["First, I want to give a shoutout to Allison Parrish (http://www.decontextualize.com/). She mixes machine learning with poetry (!) I really like her work and explanations. I borrowed her color mixing example.\n","\n","In module 6, we built a co-occurence matrix that was 25K by 25K. I'd like to use something like it and try out KNN. But it is crazy to use a collection of 25K vectors as our crowd. I'd like something much smaller. That is what we will tackle in this module, reducing the comat to a reasonable size, i.e., 300.\n","\n","But before jumping right to that, I'd like to do some build-up. There will be a bit of review here given we looked at the comat idea earlier. But I think it will give you some new insight. And will lead us toward what is called word-embeddings or word-vectors."]},{"cell_type":"markdown","metadata":{"id":"pFTFz16ncTyF","colab_type":"text"},"source":["##Might also turn on GPU\n","\n","Runtime then Change runtime type to GPU. Will use it later. This will reset your kernel so good to do it up front here."]},{"cell_type":"markdown","metadata":{"id":"R6WY5kRdvv1N","colab_type":"text"},"source":["##Before we get going\n","\n","I moved this up front so it will hang in beginning and once I set it up, I can do a Runtime/run-after and not get hung."]},{"cell_type":"code","metadata":{"id":"MDRI7dXav9WR","colab_type":"code","outputId":"cc8d4f6e-a99c-49b5-a859-0ba0d4194698","executionInfo":{"status":"ok","timestamp":1582751002560,"user_tz":480,"elapsed":17904,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":131}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n-AAgxmxBOcv"},"source":["## Animal similarity\n","\n","We'll begin by considering a small subset of English: words for animals. Our task will be similar to module 6:  find similarities among these words and the creatures they designate. So instead of similarity between `monster` and `frankenstein` we will look for similarities between `kitten` and `hamster`. I am not going to build a co-occurence matrix. Instead, I will come up with 2 features that I think are important for animals. See below:\n","\n","![Animal spreadsheet](http://static.decontextualize.com/snaps/animal-spreadsheet.png)\n","\n","This spreadsheet associates a handful of animals with two numbers: their cuteness and their size, both in a range from zero to one hundred. The values themselves are simply based on personal judgment. Your taste in cuteness and evaluation of size may differ significantly from mine. As with all data, these data are simply a mirror reflection of the person who collected them. In other words, always have to be aware of human bias in problems like this.\n","\n","Note that this is **not** a comat. Instead, it is a step toward what is called a word-vector. For instance the word `kitten` has a 2d vector 95,15.\n","\n","Let's build the pandas version of the table shown.\n"]},{"cell_type":"code","metadata":{"id":"lSGjHwKzrDJi","colab_type":"code","colab":{}},"source":["animal_dict = {\n","    'kitten': [95,15],\n","    'hamster': [80,8],\n","    'tarantula': [8,3],\n","    'puppy': [90, 20],\n","    'crocodile': [5, 40],\n","    'dolphin': [60,45],\n","    'panda bear': [75, 40],\n","    'lobster': [2, 15],\n","    'capybara': [70, 30],\n","    'elephant': [65, 90],\n","    'mosquito': [1, 1],\n","    'goldfish': [25, 2],\n","    'horse': [50, 50],\n","    'chicken': [25, 15]\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_bjpk6xRsZCB","colab_type":"code","outputId":"2b458b08-32a2-4b72-a549-049bba67f04a","executionInfo":{"status":"ok","timestamp":1582751002563,"user_tz":480,"elapsed":17890,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":478}},"source":["import pandas as pd\n","animal_table = pd.DataFrame.from_dict(animal_dict, orient='index', columns=['cuteness', 'size'])\n","animal_table"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cuteness</th>\n","      <th>size</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>kitten</th>\n","      <td>95</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>hamster</th>\n","      <td>80</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>tarantula</th>\n","      <td>8</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>puppy</th>\n","      <td>90</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>crocodile</th>\n","      <td>5</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>dolphin</th>\n","      <td>60</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>panda bear</th>\n","      <td>75</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>lobster</th>\n","      <td>2</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>capybara</th>\n","      <td>70</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>elephant</th>\n","      <td>65</td>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>mosquito</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>goldfish</th>\n","      <td>25</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>horse</th>\n","      <td>50</td>\n","      <td>50</td>\n","    </tr>\n","    <tr>\n","      <th>chicken</th>\n","      <td>25</td>\n","      <td>15</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            cuteness  size\n","kitten            95    15\n","hamster           80     8\n","tarantula          8     3\n","puppy             90    20\n","crocodile          5    40\n","dolphin           60    45\n","panda bear        75    40\n","lobster            2    15\n","capybara          70    30\n","elephant          65    90\n","mosquito           1     1\n","goldfish          25     2\n","horse             50    50\n","chicken           25    15"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"JsCi_qk3eX9n","colab_type":"text"},"source":["##New type of index\n","\n","Notice that instead of having row indices of 0,1, etc, the indices are now words. We can still get the 0th row using iloc as shown below."]},{"cell_type":"code","metadata":{"id":"i0swJ-Kceq9W","colab_type":"code","colab":{}},"source":["row0 = animal_table.iloc[0]  #gets first row no matter what the index"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZoklWQ8wfaOi","colab_type":"code","outputId":"8b34b502-b560-4c0e-da84-0a10fba8d943","executionInfo":{"status":"ok","timestamp":1582751002563,"user_tz":480,"elapsed":17874,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["row0"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["cuteness    95\n","size        15\n","Name: kitten, dtype: int64"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"aTndZOp_hkmF","colab_type":"code","outputId":"40c5dbc3-7584-451f-e027-6104eeb0ff79","executionInfo":{"status":"ok","timestamp":1582751002564,"user_tz":480,"elapsed":17865,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["type(row0)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pandas.core.series.Series"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"rpUsTjAlfcwt","colab_type":"code","outputId":"812641c1-4c04-45b1-deb3-bce7f778a936","executionInfo":{"status":"ok","timestamp":1582751002564,"user_tz":480,"elapsed":17856,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["row0.name  #a bit weird - series object has a name field taken from index"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'kitten'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"MdmVx6AIfJB8","colab_type":"code","outputId":"2213b202-3919-45c1-86fc-85cf109413c4","executionInfo":{"status":"ok","timestamp":1582751002564,"user_tz":480,"elapsed":17846,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["animal_table.columns  #but name is not in table"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['cuteness', 'size'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"7LyUgoSKewKb","colab_type":"text"},"source":["Below will fail if you uncomment it and run it. Why? The loc method assumes you give it column names. So the 0 should be in the column index. This is confusing because pandas hides this column name. It just shows it on the left. I see `kitten` in the index but no 0."]},{"cell_type":"code","metadata":{"id":"P7NOqnNEeydh","colab_type":"code","colab":{}},"source":["#animal_table.loc[0, 'size']  #looks for index 0 and does not see it"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"97paz6WqdwzG","colab_type":"code","outputId":"50288ca0-579b-463f-e14e-d22303ae8bc3","executionInfo":{"status":"ok","timestamp":1582751002735,"user_tz":480,"elapsed":18002,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":92}},"source":["animal_table.index"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['kitten', 'hamster', 'tarantula', 'puppy', 'crocodile', 'dolphin',\n","       'panda bear', 'lobster', 'capybara', 'elephant', 'mosquito', 'goldfish',\n","       'horse', 'chicken'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"JJFjaaPGeWUr","colab_type":"code","outputId":"0676bd03-60b9-46a8-f8a4-b72f52a8bc0a","executionInfo":{"status":"ok","timestamp":1582751002736,"user_tz":480,"elapsed":17993,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["animal_table.loc['kitten']  #finds row with index kitten"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["cuteness    95\n","size        15\n","Name: kitten, dtype: int64"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"AKHEkKNGevvz","colab_type":"code","outputId":"37cc763a-62d0-4609-be4e-34ea11d6250d","executionInfo":{"status":"ok","timestamp":1582751002736,"user_tz":480,"elapsed":17983,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["animal_table.loc['kitten', 'size']  #finds row with index kitten and pulls out size value"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"ecisZirGM9wT","colab_type":"text"},"source":["Personally, I find the way pandas handles dataframes and series object a bit confusing. We have to live with it."]},{"cell_type":"markdown","metadata":{"id":"hAJJP6swrBuX","colab_type":"text"},"source":["\n","The values in the table give us a way to make determinations about which animals are similar. For example, try to answer the following question: Which animal is most similar to a `capybara`? You could go through the values one by one and use euclidean distance (or cosine similarity) to make that evaluation. This is quite similar to what we were doing with `ordered_distances`.\n","\n","Let's try visualizing the data as points in 2-dimensional space:\n","\n","![Animal space](http://static.decontextualize.com/snaps/animal-space.png)\n"]},{"cell_type":"markdown","metadata":{"id":"WbOTItEbqBO4","colab_type":"text"},"source":["##Bring in your library now"]},{"cell_type":"code","metadata":{"id":"zpw-rngvsbwO","colab_type":"code","outputId":"22e29b99-c7fe-42ba-c56d-bab30ecdbc45","executionInfo":{"status":"ok","timestamp":1582751006333,"user_tz":480,"elapsed":21571,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#your code\n","!rm -r  'w20_ds_library'\n","clone_url = f'https://github.com/FutureDeus/w20_ds_library.git'\n","!git clone $clone_url\n","from w20_ds_library import *"],"execution_count":13,"outputs":[{"output_type":"stream","text":["rm: cannot remove 'w20_ds_library': No such file or directory\n","Cloning into 'w20_ds_library'...\n","remote: Enumerating objects: 102, done.\u001b[K\n","remote: Counting objects: 100% (102/102), done.\u001b[K\n","remote: Compressing objects: 100% (101/101), done.\u001b[K\n","remote: Total 102 (delta 64), reused 0 (delta 0), pack-reused 0\u001b[K\n","Receiving objects: 100% (102/102), 23.62 KiB | 5.91 MiB/s, done.\n","Resolving deltas: 100% (64/64), done.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uLl2huwYtj_b","colab_type":"text"},"source":["Ok, let's check out the panda capybara distance. I am using euclidean distance because I think it is easier to visualize on the graph."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8IJjJNywBOc1","outputId":"fc07587b-4fb0-4cb9-c4b2-e00dec8ce5da","executionInfo":{"status":"ok","timestamp":1582751006333,"user_tz":480,"elapsed":21560,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["euclidean_distance([70, 30], [75, 40]) # panda and capybara  11.180339887498949"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11.180339887498949"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5qmw4wFLBOc8"},"source":["It is less than the distance between \"tarantula\" and \"elephant\":"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IHBJ2l61BOc9","outputId":"649c36a5-526f-403b-c244-f016e58a309f","executionInfo":{"status":"ok","timestamp":1582751006333,"user_tz":480,"elapsed":21554,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["euclidean_distance([8, 3], [65, 90]) # tarantula and elephant  104.0096149401583"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["104.0096149401583"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fsO1_hxpBOdB"},"source":["Modeling animals in this way has a few other interesting properties. For example, you can pick an arbitrary point in \"animal space\" and then find the animal closest to that point. If you imagine an animal of size 25 and cuteness 30, you can easily look at the space to find the animal that most closely fits that description: the chicken.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7OcD3PEm5_Yq","colab_type":"text"},"source":["I am going to write a special function to work with the animal table. I could probably rework my existing `ordered_distances` function, but decided easier to write this new one."]},{"cell_type":"code","metadata":{"id":"tV5wf3sKqtfV","colab_type":"code","colab":{}},"source":["def ordered_animals(target_vector, table):\n","  #distance_list = [(row.name,euclidean_distance(target_vector, row.tolist())) for i,row in table.iterrows()]  #for list comprehension fans\n","  distance_list = []\n","  for i,row in table.iterrows():\n","    a_vector = row.tolist()\n","    d = euclidean_distance(target_vector, a_vector)\n","    distance_list.append((row.name,d))\n","  return sorted(distance_list, key=lambda pair: pair[1], reverse=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6TPlN_zBxB_q","colab_type":"code","outputId":"705ddbf0-f660-411b-d2dc-1f8b1f6cc9bb","executionInfo":{"status":"ok","timestamp":1582751006334,"user_tz":480,"elapsed":21541,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":279}},"source":["pup = animal_table.loc['puppy'].tolist()\n","\n","ordered_animals(pup, animal_table)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('puppy', 0.0),\n"," ('kitten', 7.0710678118654755),\n"," ('hamster', 15.620499351813308),\n"," ('capybara', 22.360679774997898),\n"," ('panda bear', 25.0),\n"," ('dolphin', 39.05124837953327),\n"," ('horse', 50.0),\n"," ('chicken', 65.19202405202648),\n"," ('goldfish', 67.446274915669),\n"," ('elephant', 74.33034373659252),\n"," ('tarantula', 83.74365647617735),\n"," ('crocodile', 87.32124598286491),\n"," ('lobster', 88.14193099768123),\n"," ('mosquito', 91.00549433962765)]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"g1CoTvMWqqkf","colab_type":"text"},"source":["Let's look at it  geometrically. You can  answer questions like: what's halfway between a chicken and an elephant? Simply draw a line from \"elephant\" to \"chicken,\" mark off the midpoint and find the closest animal. (According to our chart, halfway between an elephant and a chicken is a horse.) Let's check that out computationally.\n","\n"]},{"cell_type":"code","metadata":{"id":"bWPggzT8zAsv","colab_type":"code","outputId":"12337c05-a068-4455-846c-a72abb1e840e","executionInfo":{"status":"ok","timestamp":1582751006334,"user_tz":480,"elapsed":21534,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["elephant = animal_table.loc['elephant'].tolist()\n","chicken = animal_table.loc['chicken'].tolist()\n","half_vector = [(e+c)/2  for e,c in zip(elephant, chicken)]\n","half_vector  #[45.0, 52.5]"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[45.0, 52.5]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"zD1g7ypc0VH9","colab_type":"code","outputId":"60e30568-1fc4-419b-bc78-5e4b5d8f6b0b","executionInfo":{"status":"ok","timestamp":1582751006334,"user_tz":480,"elapsed":21526,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":279}},"source":["ordered_animals(half_vector, animal_table)"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('horse', 5.5901699437494745),\n"," ('dolphin', 16.77050983124842),\n"," ('panda bear', 32.5),\n"," ('capybara', 33.63406011768428),\n"," ('crocodile', 41.907636535600524),\n"," ('elephant', 42.5),\n"," ('chicken', 42.5),\n"," ('goldfish', 54.31620384378864),\n"," ('puppy', 55.509008277936296),\n"," ('hamster', 56.61492736019362),\n"," ('lobster', 57.05479822065801),\n"," ('tarantula', 61.80008090609591),\n"," ('kitten', 62.5),\n"," ('mosquito', 67.73662229547618)]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"Xu3bbkfNy6aT","colab_type":"text"},"source":["You can also ask: what's the *difference* between a hamster and a tarantula? According to our plot, it's about seventy five units of cute (and a few units of size).\n","\n","The relationship of \"difference\" is an interesting one, because it allows us to reason about *analogous* relationships. In the chart below, I've drawn an arrow from \"tarantula\" to \"hamster\" (in blue):\n","\n","![Animal analogy](http://static.decontextualize.com/snaps/animal-space-analogy.png)\n","\n","You can understand this arrow as being the *relationship* between a tarantula and a hamster, in terms of their size and cuteness (i.e., hamsters and tarantulas are about the same size, but hamsters are much cuter). In the same diagram, I've also transposed this same arrow (this time in red) so that its origin point is \"chicken.\" The arrow ends closest to \"kitten.\" What we've discovered is that the animal that is about the same size as a chicken but much cuter is... a kitten. To put it in terms of an analogy:\n","\n","    Tarantulas are to hamsters as chickens are to kittens.\n","    \n"]},{"cell_type":"code","metadata":{"id":"pkwYUrce1CnO","colab_type":"code","outputId":"6275f362-4acb-4629-80d4-39ca90513e59","executionInfo":{"status":"ok","timestamp":1582751006334,"user_tz":480,"elapsed":21520,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["tarantula = animal_table.loc['tarantula'].tolist()\n","hamster = animal_table.loc['hamster'].tolist()\n","thd = euclidean_distance(tarantula, hamster)  #tarantula hamster diff\n","thd  #72.17340230306452"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["72.17340230306452"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"vxRAV6_FPa9r","colab_type":"text"},"source":["Now get animal distances from chicken and find the one closest to 72.17."]},{"cell_type":"code","metadata":{"id":"sGPM37A71dur","colab_type":"code","outputId":"4b2b5570-1ee3-43e8-bbb1-5d15f0e807f0","executionInfo":{"status":"ok","timestamp":1582751006335,"user_tz":480,"elapsed":21515,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":279}},"source":["chick_ds = ordered_animals(chicken, animal_table)\n","chick_ds"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('chicken', 0.0),\n"," ('goldfish', 13.0),\n"," ('tarantula', 20.808652046684813),\n"," ('lobster', 23.0),\n"," ('mosquito', 27.784887978899608),\n"," ('crocodile', 32.01562118716424),\n"," ('horse', 43.01162633521314),\n"," ('dolphin', 46.09772228646444),\n"," ('capybara', 47.43416490252569),\n"," ('hamster', 55.44366510251645),\n"," ('panda bear', 55.90169943749474),\n"," ('puppy', 65.19202405202648),\n"," ('kitten', 70.0),\n"," ('elephant', 85.0)]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"SMm_N-Hn3b0Q","colab_type":"code","outputId":"62b7d188-04a8-40bb-d8ac-5d875217dfb0","executionInfo":{"status":"ok","timestamp":1582751006335,"user_tz":480,"elapsed":21508,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["min(chick_ds, key=lambda x:abs(x[1]-thd))  #('kitten', 70.0)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('kitten', 70.0)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"ElVVVkly1BsF","colab_type":"text"},"source":["\n","A set of vectors that are all part of the same data set is often called a *vector space*, e.g., the crowd table in KNN. The vector space of animals in this section has two *dimensions*, by which I mean that each vector in the space has two numbers associated with it (i.e., two columns in the spreadsheet). The fact that this space has two dimensions just happens to make it easy to *visualize* the space by drawing a 2D plot. But most vector spaces you'll work with will have more than two dimensions—sometimes many hundreds. In those cases, it's more difficult to visualize the \"space,\" but the math works pretty much the same."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jWi-8rZtBOdD"},"source":["## Language with vectors: colors\n","\n","So far, so good. We have a system in place—albeit highly subjective—for talking about animals and the words used to name them. I want to talk about another vector space that has to do with language: the vector space of colors.\n","\n","Colors are often represented in computers as vectors with three dimensions: red, green, and blue. Just as with the animals in the previous section, we can use these vectors to answer questions like: which colors are similar? What's the most likely color name for an arbitrarily chosen set of values for red, green and blue? Given the names of two colors, what's the name of those colors' \"average\"?\n","\n","We'll be working with this [color data](https://github.com/dariusk/corpora/blob/master/data/colors/xkcd.json) from the [xkcd color survey](https://blog.xkcd.com/2010/05/03/color-survey-results/). The data relates a color name to the RGB value associated with that color. [Here's a page that shows what the colors look like](https://xkcd.com/color/rgb/). Read the color data now."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Uk-ZwCTVBOdE","colab":{}},"source":["import json\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GY8GGw0m1G_N","colab_type":"text"},"source":["Here is link to json file. You will have to upload it to Google Drive and then read it: https://drive.google.com/open?id=1Zo35tEkTN9Z_G-YIPN7u6LaoXYEHqio-. So replace my path with your own."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"h071tUIsBOdI","colab":{}},"source":["color_data = json.loads(open(\"/content/gdrive/My Drive/class_tables/xkcd.json\").read())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gI8yz-tkBOdL"},"source":["The following function converts colors from hex format (`#1a2b3c`) to a tuple of integers:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zs4nhn46BOdM","colab":{}},"source":["def hex_to_int(s:str) -> tuple:\n","  assert isinstance(s, str)\n","  assert s[0] == '#'\n","  assert len(s) == 7\n","\n","  s = s.lstrip(\"#\")\n","  return int(s[:2], 16), int(s[2:4], 16), int(s[4:6], 16)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y0vLRf1wQwaT","colab_type":"code","outputId":"9fa9390a-f9f7-4855-f6dd-2df995e991ac","executionInfo":{"status":"ok","timestamp":1582751006774,"user_tz":480,"elapsed":21921,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["hex_to_int('#1a2b3c')  #(26, 43, 60)"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(26, 43, 60)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9YYqsA7pBOdR"},"source":["And the following cell creates a dictionary and populates it with mappings from color names to RGB vectors for each color in the data:"]},{"cell_type":"code","metadata":{"id":"llnD-SGLKPNK","colab_type":"code","colab":{}},"source":["colors = {item[\"color\"]: hex_to_int(item[\"hex\"]) for item in color_data['colors']}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R9AZSkS9BOdY"},"source":["Testing it out:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7t9g8wNJBOdZ","outputId":"56f3350c-f4ed-4bb6-8975-96cdf49c638c","executionInfo":{"status":"ok","timestamp":1582751006774,"user_tz":480,"elapsed":21908,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["colors['olive']  #(110, 117, 14)"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(110, 117, 14)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CP2yyGdQBOde","outputId":"38983dd4-839c-445e-d49c-29f7c436320c","executionInfo":{"status":"ok","timestamp":1582751006775,"user_tz":480,"elapsed":21902,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["colors['red']  #(229, 0, 0)"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(229, 0, 0)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"__eM9nMLBOdk","outputId":"ff4de6cc-d035-4adb-f4b4-dae91ad1ee50","executionInfo":{"status":"ok","timestamp":1582751006775,"user_tz":480,"elapsed":21896,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["colors['black']  #(0, 0, 0)"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 0, 0)"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gvUfJaZHBOdo"},"source":["#Challenge 1\n","\n","We will need some functions for performing basic vector \"arithmetic.\" These functions will work with vectors in spaces of any number of dimensions.\n","\n","Here is where you finally get to use `numpy`. Please take my Python code and find the `numpy` equivalent of doing things.\n","\n","Except for `fast_cosine`, all of my numpy functions were 1 line long. If you are getting longer, search a little harder in the numpy documentation."]},{"cell_type":"markdown","metadata":{"id":"IzVoEecvUhq8","colab_type":"text"},"source":["First I will add some things to do type hints."]},{"cell_type":"code","metadata":{"id":"ByaNK9A8TCdd","colab_type":"code","colab":{}},"source":["import numpy\n","from typing import TypeVar, Callable\n","narray = TypeVar('numpy.ndarray')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pe7ASWaw9Yd8","colab_type":"text"},"source":["##Numpy equivalent of this please"]},{"cell_type":"code","metadata":{"id":"vfN7mKLQ91ID","colab_type":"code","colab":{}},"source":["a = [10,1]\n","b = [5,2]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ydQ_Ypuu9Kxa","colab_type":"code","outputId":"f330812b-af1a-424d-8b8b-24fffed7495c","executionInfo":{"status":"ok","timestamp":1582751006962,"user_tz":480,"elapsed":22062,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["euclidean_distance(a,b)  #5.0990195135927845"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5.0990195135927845"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"f1yOQ1EvuWrW","colab_type":"code","colab":{}},"source":["import numpy as np  #standard way to do numpy import"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x2nScTqiUW7p","colab_type":"code","colab":{}},"source":["def fast_euclidean_distance(x:narray, y:narray) -> float:\n","  assert isinstance(x, numpy.ndarray), f\"x must be a numpy array but instead is {type(x)}\"\n","  assert len(x.shape) == 1, f\"x must be a 1d array but instead is {len(x.shape)}d\"\n","  assert isinstance(y, numpy.ndarray), f\"y must be a numpy array but instead is {type(y)}\"\n","  assert len(y.shape) == 1, f\"y must be a 1d array but instead is {len(y.shape)}d\"\n","  \n","  return np.linalg.norm(x-y)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aadbQzv89_SL","colab_type":"code","colab":{}},"source":["na = np.array(a)\n","nb = np.array(b)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6EkHmsd89yGp","colab_type":"code","outputId":"682e4a9c-ffc0-458c-f602-5c4da20f514d","executionInfo":{"status":"ok","timestamp":1582751006963,"user_tz":480,"elapsed":22037,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["fast_euclidean_distance(na,nb)  #5.0990195135927845\n"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5.0990195135927845"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gNhxQ55s-TzW"},"source":["##Numpy equivalent of this please"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sPZYhkrhBOdt","outputId":"a2706a92-94b8-436c-b585-5a2c61aba797","executionInfo":{"status":"ok","timestamp":1582751006963,"user_tz":480,"elapsed":22031,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["[c1 - c2 for c1, c2 in zip(a, b)]  #subtract one vector from another\n"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[5, -1]"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"zSDmHKuBVShY","colab_type":"code","colab":{}},"source":["def subtractv(x:narray, y:narray) -> narray:\n","  assert isinstance(x, numpy.ndarray), f\"x must be a numpy array but instead is {type(x)}\"\n","  assert len(x.shape) == 1, f\"x must be a 1d array but instead is {len(x.shape)}d\"\n","  assert isinstance(y, numpy.ndarray), f\"y must be a numpy array but instead is {type(y)}\"\n","  assert len(y.shape) == 1, f\"y must be a 1d array but instead is {len(y.shape)}d\"\n","\n","  return np.subtract(x, y)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"No6eFaOQ-f7M","colab_type":"code","outputId":"8b2ea02e-1560-4b64-fc01-ca6ffcce61c9","executionInfo":{"status":"ok","timestamp":1582751006964,"user_tz":480,"elapsed":22019,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["subtractv(na,nb)  #array([ 5, -1])"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 5, -1])"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TouLvo7W-46r"},"source":["##Numpy equivalent of this please"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"489873ca-88a0-4b60-c861-d8080993a4d1","id":"97uvl1oZ-46t","executionInfo":{"status":"ok","timestamp":1582751006964,"user_tz":480,"elapsed":22013,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["[c1 + c2 for c1, c2 in zip(a, b)]  #add one vector to another\n"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[15, 3]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"l9nOp6_bVp8X","colab_type":"code","colab":{}},"source":["def addv(x:narray, y:narray) -> narray:\n","  assert isinstance(x, numpy.ndarray), f\"x must be a numpy array but instead is {type(x)}\"\n","  assert len(x.shape) == 1, f\"x must be a 1d array but instead is {len(x.shape)}d\"\n","  assert isinstance(y, numpy.ndarray), f\"y must be a numpy array but instead is {type(y)}\"\n","  assert len(y.shape) == 1, f\"y must be a 1d array but instead is {len(y.shape)}d\"\n","  \n","  return np.add(x, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"c6053f1f-ba5e-4f60-a624-64a397929dc7","id":"YL1iFiJa-46w","executionInfo":{"status":"ok","timestamp":1582751006965,"user_tz":480,"elapsed":22002,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["addv(na,nb)  #array([15,  3])"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([15,  3])"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wniz-Ipn_Xgj"},"source":["##Numpy equivalent of this please"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P7sozCCIBOd5"},"source":["My `meanv_slow` function takes a list of vectors and finds their mean or average. Provide numpy equivalent."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RkCzg2MhBOd6","colab":{}},"source":["def meanv_slow(coords):\n","    # assumes every item in coords has same length as item 0\n","    sumv = [0] * len(coords[0])\n","    for item in coords:\n","        for i in range(len(item)):\n","            sumv[i] += item[i]\n","    mean = [0] * len(sumv)\n","    for i in range(len(sumv)):\n","        mean[i] = float(sumv[i]) / len(coords)\n","    return mean\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nodrozbt_u_w","colab_type":"code","colab":{}},"source":["test = [[0, 1], [2, 2], [4, 3]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6CSbmooO_pWT","colab_type":"code","outputId":"5b423105-95d9-42f9-f053-3ff94a065054","executionInfo":{"status":"ok","timestamp":1582751006965,"user_tz":480,"elapsed":21984,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["meanv_slow(test)  #[2.0, 2.0]"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2.0, 2.0]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"wAw-M9YLWFn9","colab_type":"code","colab":{}},"source":["def meanv(matrix: narray) -> narray:\n","  assert isinstance(matrix, numpy.ndarray), f\"matrix must be a numpy array but instead is {type(matrix)}\"\n","  assert len(matrix.shape) == 2, f\"matrix must be a 2d array but instead is {len(matrix.shape)}d\"\n","\n","  return np.mean(matrix, axis = 0)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQT8Fsm4AD78","colab_type":"code","outputId":"aa91eff6-102d-4d8e-f2fb-668076b7c322","executionInfo":{"status":"ok","timestamp":1582751007199,"user_tz":480,"elapsed":22204,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["ntest = np.array(test)\n","ntest"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1],\n","       [2, 2],\n","       [4, 3]])"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"tOO75tnBAtSB","colab_type":"code","outputId":"8391030e-53c2-4f5e-e7d3-d139124acc18","executionInfo":{"status":"ok","timestamp":1582751007199,"user_tz":480,"elapsed":22197,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["meanv(ntest)  #array([2., 2.])"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2., 2.])"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"J-kHa2e1VJ-3"},"source":["##Numpy equivalent of this please\n","\n","You have a cosine_similarity function now. Create a new version called fast_cosine using numpy. Be careful to look out for 0-divide in the denominator!"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8sav0XjCVA9G","colab":{}},"source":["from numpy.linalg import norm  #hint: i found this useful\n","\n","def fast_cosine(v1:narray, v2:narray) -> float:\n","  assert isinstance(v1, numpy.ndarray), f\"v1 must be a numpy array but instead is {type(v1)}\"\n","  assert len(v1.shape) == 1, f\"v1 must be a 1d array but instead is {len(v1.shape)}d\"\n","  assert isinstance(v2, numpy.ndarray), f\"v2 must be a numpy array but instead is {type(v2)}\"\n","  assert len(v2.shape) == 1, f\"v2 must be a 1d array but instead is {len(v2.shape)}d\"\n","  assert len(v1) == len(v2), f'v1 and v2 must have same length but instead have {len(v1)} and {len(v2)}'\n","\n","  bottom = (norm(v1)*norm(v2))\n","  return 0.0 if bottom == 0 else np.dot(v1, v2)/bottom\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvNWje8oXECx","colab_type":"code","outputId":"6fb82a64-d61d-4855-f53b-fe1dc7575b7e","executionInfo":{"status":"ok","timestamp":1582751007200,"user_tz":480,"elapsed":22185,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["fast_cosine(np.array(colors['olive']), np.array(colors['red']))  #0.6823879113063314"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6823879113063314"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"Y1flRkCpXyng","colab_type":"code","outputId":"2554f6b1-a7db-461a-9cce-3f9f2f89b455","executionInfo":{"status":"ok","timestamp":1582751007200,"user_tz":480,"elapsed":22179,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["fast_cosine(np.array(colors['black']), np.array(colors['red']))  #0.0"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"XCpsKVb2B-qt","colab_type":"text"},"source":["#Challenge 2\n","\n","Let's make life easier for ourselves. Convert colors and animal_dict to numpy versions and then define a new version of ordered_distances."]},{"cell_type":"code","metadata":{"id":"2v3BBFkNCtZZ","colab_type":"code","colab":{}},"source":["#your code here\n","\n","n_colors = {keys:np.array(values) for keys, values in colors.items()}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RvxsxHqsDfeK","colab_type":"code","outputId":"64881dc8-8d39-49ec-9209-5e3ccfce73f8","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751007200,"user_tz":480,"elapsed":22166,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["n_colors['olive']  #array([110, 117,  14])"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([110, 117,  14])"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"HtDuGRIZvML_","colab_type":"code","outputId":"f5b2aaea-ab44-474e-d52e-db5746f03992","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751007201,"user_tz":480,"elapsed":22161,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["type(n_colors['olive'])  #numpy.ndarray"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"Tzg6DYPtC_kT","colab_type":"code","colab":{}},"source":["#your code here\n","\n","n_animal_dict = {keys:np.array(values) for keys, values in animal_dict.items()}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ksZuI-U0Dsup","colab_type":"code","outputId":"a3ea1882-39e3-4a1a-9c10-7fb016f415cc","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751007201,"user_tz":480,"elapsed":22149,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["n_animal_dict['chicken']  #array([25, 15])"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([25, 15])"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"B5TjwWBuNAAm","colab_type":"code","outputId":"4a78831b-ff1e-4b16-b3c8-8a7c7199a651","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751007201,"user_tz":480,"elapsed":22143,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["type(n_animal_dict['chicken'])  #numpy.ndarray"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZVkOJQrLBOd-"},"source":["Just as a test, use numpy to show that the distance from \"red\" to \"green\" is greater than the distance from \"red\" to \"pink\"."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ie3AZcOZBOd_","outputId":"e2e2a51e-d66e-47cd-b646-deadc4b7df75","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751007202,"user_tz":480,"elapsed":22137,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["fast_euclidean_distance(n_colors['red'],n_colors['green']) > fast_euclidean_distance(n_colors['red'],n_colors['pink'])  #True"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fJLtvQWiBOeE"},"source":["### Finding the closest item\n","\n","Just as we wanted to find the animal that most closely matched an arbitrary point in cuteness/size space, we'll want to find the closest color name to an arbitrary point in RGB space. The easiest way to find the closest item to an arbitrary vector is simply to find the distance between the target vector and each item in the space, in turn, then sort the list from closest to farthest. \n","\n","Note we want distances so use your fast_euclidean_distance function."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-AfFjW76BOeG","colab":{}},"source":["def dict_ordered_distances(space:dict, coord:narray) -> list:\n","  assert isinstance(space, dict), f\"space must be a dictionary but instead a {type(space)}\"\n","  assert isinstance(list(space.values())[0], numpy.ndarray), f\"space must have numpy arrays as values but instead has {type(space.values()[0])}\"\n","  assert isinstance(coord, numpy.ndarray), f\"coord must be a numpy array but instead is {type(cord)}\"\n","  assert len(list(space.values())[0]) == len(coord), f\"space values must be same length as coord\"\n","  assert len(coord) == 3, \"coord must be a triple\"\n","\n","  #your code here\n","  return sorted([(keys, fast_euclidean_distance(values, coord)) for keys, values in space.items()], key = lambda x: x[1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KC_yzYc3BOeK"},"source":["Testing it out, we can find the ten colors closest to \"red\":"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GVJzS-aoBOeL","outputId":"3d478c99-1fa7-409d-8cb5-825ce4146a2e","scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1582751007202,"user_tz":480,"elapsed":22125,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["dict_ordered_distances(n_colors, n_colors['red'])[:10]"],"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('red', 0.0),\n"," ('fire engine red', 25.079872407968907),\n"," ('bright red', 29.068883707497267),\n"," ('tomato red', 45.552167895721496),\n"," ('cherry red', 45.73838650411709),\n"," ('scarlet', 46.33573135281238),\n"," ('vermillion', 53.563046963368315),\n"," ('orangish red', 56.2672195865408),\n"," ('cherry', 56.49778756730214),\n"," ('lipstick red', 59.84981202978001)]"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"qH0g9Q2Bv9EM","colab_type":"text"},"source":["<pre>\n","[('red', 0.0),\n"," ('fire engine red', 25.079872407968907),\n"," ('bright red', 29.068883707497267),\n"," ('tomato red', 45.552167895721496),\n"," ('cherry red', 45.73838650411709),\n"," ('scarlet', 46.33573135281238),\n"," ('vermillion', 53.563046963368315),\n"," ('orangish red', 56.2672195865408),\n"," ('cherry', 56.49778756730214),\n"," ('lipstick red', 59.84981202978001)]\n"," </pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TYiR9aHmBOeQ"},"source":["... or the ten colors closest to (150, 60, 150):"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qZ6-yNzQBOeR","outputId":"776feea4-9266-4951-c5ae-70a309326818","scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1582751007202,"user_tz":480,"elapsed":22119,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["dict_ordered_distances(n_colors, np.array([150, 60, 150]))[:10]"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('warm purple', 15.684387141358123),\n"," ('medium purple', 16.0312195418814),\n"," ('ugly purple', 18.2208671582886),\n"," ('light eggplant', 23.2163735324878),\n"," ('purpleish', 27.586228448267445),\n"," ('purplish', 27.92848008753788),\n"," ('purply', 28.231188426986208),\n"," ('light plum', 33.74907406137241),\n"," ('purple', 38.88444419044716),\n"," ('muted purple', 40.8656334834051)]"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"markdown","metadata":{"id":"SpXOevRRwExs","colab_type":"text"},"source":["<pre>\n","[('warm purple', 15.684387141358123),\n"," ('medium purple', 16.0312195418814),\n"," ('ugly purple', 18.2208671582886),\n"," ('light eggplant', 23.2163735324878),\n"," ('purpleish', 27.586228448267445),\n"," ('purplish', 27.92848008753788),\n"," ('purply', 28.231188426986208),\n"," ('light plum', 33.74907406137241),\n"," ('purple', 38.88444419044716),\n"," ('muted purple', 40.8656334834051)]\n"," </pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BTuZLO4xBOeW"},"source":["### Color magic\n","\n","The magical part of representing words as vectors is that the vector operations we defined earlier appear to operate on language the same way they operate on numbers. For example, if we find the word closest to the vector resulting from subtracting \"red\" from \"purple,\" we get a series of \"blue\" colors:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IhOzalOGBOeX","outputId":"e343d625-e317-4ada-af32-8a26c6647df7","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1582751007203,"user_tz":480,"elapsed":22113,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["dict_ordered_distances(n_colors, subtractv(n_colors['purple'], n_colors['red']))[:10]"],"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('cobalt blue', 108.42970072816765),\n"," ('royal blue', 111.96428001822724),\n"," ('darkish blue', 112.76967677527502),\n"," ('true blue', 115.52056094046635),\n"," ('royal', 115.56383517346592),\n"," ('prussian blue', 116.18519699169941),\n"," ('dark royal blue', 116.73045875006231),\n"," ('deep blue', 117.957619508025),\n"," ('marine blue', 118.28778466096996),\n"," ('deep sea blue', 120.03332870498926)]"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"stEmJMpuBOec"},"source":["This matches our intuition about RGB colors, which is that purple is a combination of red and blue. Take away the red, and blue is all you have left.\n","\n","You can do something similar with addition. What's blue plus green?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uNEdHe7vBOed","outputId":"de94b5bf-3f17-4c5b-810e-e8d82787a94c","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1582751007451,"user_tz":480,"elapsed":22355,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["dict_ordered_distances(n_colors, addv(n_colors['blue'], n_colors['green']))[:10]"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('bright turquoise', 14.212670403551895),\n"," ('bright light blue', 15.0996688705415),\n"," ('bright aqua', 20.73644135332772),\n"," ('cyan', 27.49545416973504),\n"," ('neon blue', 33.34666400106613),\n"," ('aqua blue', 38.3275357934736),\n"," ('bright cyan', 42.49705872175156),\n"," ('bright sky blue', 45.05552130427524),\n"," ('aqua', 49.09175083453431),\n"," ('bright teal', 56.2672195865408)]"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5m_H7XwdBOei"},"source":["That's right, it's something like turquoise or cyan!\n"]},{"cell_type":"markdown","metadata":{"id":"tuaP-FHUSyVg","colab_type":"text"},"source":["#Challenge 3\n","\n"," What if we find the average of black and white? Predictably, we get gray:\n","\n"," <pre>\n"," [u'medium grey',\n"," u'purple grey',\n"," u'steel grey',\n"," u'battleship grey',\n"," u'grey purple',\n"," u'purplish grey',\n"," u'greyish purple',\n"," u'steel',\n"," u'warm grey',\n"," u'green grey']\n"," </pre>"]},{"cell_type":"markdown","metadata":{"id":"vDCgjJoxwpaS","colab_type":"text"},"source":["Let's make sure my answer is correct. First, build your matrix with the black row and the white row."]},{"cell_type":"code","metadata":{"id":"57tC8KbvTDDK","colab_type":"code","colab":{}},"source":["#your code below\n","\n","bw_matrix = np.array([n_colors['black'], n_colors['white']])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y069daWRwwxu","colab_type":"code","outputId":"aa0a8962-2d48-4193-b11d-4cec8919fee4","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1582751007451,"user_tz":480,"elapsed":22342,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["bw_matrix  #array([[  0,   0,   0], [255, 255, 255]])"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  0,   0,   0],\n","       [255, 255, 255]])"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"v5tGqWUcUAgv","colab_type":"text"},"source":["<pre>\n","array([[  0,   0,   0],\n","       [255, 255, 255]])\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"soUuP1lGw1M6","colab_type":"text"},"source":["Now take the mean and find the closest colors to that mean."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7DftbM8nBOej","colab":{}},"source":["#your code here\n","\n","bw_list = dict_ordered_distances(n_colors, meanv(bw_matrix))[:10]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_eRwrLOxIqZ","colab_type":"code","outputId":"f6463932-8c5c-40e7-a5f7-130c231e7446","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1582751007452,"user_tz":480,"elapsed":22331,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["bw_list  #make sure matches list above"],"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('medium grey', 4.330127018922194),\n"," ('purple grey', 18.567444627627143),\n"," ('steel grey', 19.716744153130353),\n"," ('battleship grey', 21.511624764298954),\n"," ('grey purple', 22.46664193866097),\n"," ('purplish grey', 24.14021540914662),\n"," ('greyish purple', 24.264171117101856),\n"," ('steel', 25.470571253900058),\n"," ('warm grey', 26.12948526090784),\n"," ('green grey', 26.205915362757317)]"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8HKPooogBOem"},"source":["Just as with the tarantula/hamster example from the previous section, we can use color vectors to reason about relationships between colors. In the cell below, finding the difference between \"pink\" and \"red\" then adding it to \"blue\" seems to give us a list of colors that are to blue what pink is to red (i.e., a slightly lighter, less saturated shade):"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XqzWwgg6BOen","outputId":"102432f5-04af-42e5-812f-0859bf5f3de4","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1582751007452,"user_tz":480,"elapsed":22325,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["# an analogy: pink is to red as X is to blue\n","pink_to_red = subtractv(n_colors['pink'], n_colors['red'])\n","dict_ordered_distances(n_colors, addv(pink_to_red, n_colors['blue']))[:10]"],"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('neon blue', 163.29727493133498),\n"," ('bright sky blue', 163.44418007380992),\n"," ('bright light blue', 170.0764533967004),\n"," ('cyan', 172.97976760303501),\n"," ('bright cyan', 174.54512310574592),\n"," ('bright turquoise', 176.39727889057698),\n"," ('clear blue', 178.23860412379804),\n"," ('azure', 178.54131174604942),\n"," ('dodger blue', 178.92456511055155),\n"," ('lightish blue', 180.95303258028034)]"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_BNuFCvIBOer"},"source":["Another example of color analogies: Navy is to blue as true green/dark grass green is to green:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UaBwjzRKBOes","outputId":"8cccd9b9-48fb-40e4-dccc-95dafae28a84","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1582751007452,"user_tz":480,"elapsed":22319,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["navy_to_blue = subtractv(n_colors['navy'], n_colors['blue'])\n","dict_ordered_distances(n_colors, addv(navy_to_blue, n_colors['green']))[:10]"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('true green', 140.59160714637272),\n"," ('dark grass green', 143.85409274678284),\n"," ('grassy green', 147.770091696527),\n"," ('racing green', 148.82540105774956),\n"," ('forest', 151.07944929738127),\n"," ('bottle green', 151.52887513606112),\n"," ('dark olive green', 153.4079528577316),\n"," ('darkgreen', 153.6522046701576),\n"," ('forrest green', 154.042202009709),\n"," ('grass green', 154.52184311611094)]"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"C08ApLJQBOew"},"source":["The examples above are fairly simple from a mathematical perspective but nevertheless *feel* magical: they're demonstrating that it's possible to use math to reason about how people use language."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WSnnTXt4BOe1"},"source":["### Doing bad digital humanities with color vectors\n","\n","With the tools above in hand, we can start using our vectorized knowledge of language toward academic ends. In the following example, I'm going to calculate the average color of Bram Stoker's *Dracula*.\n","\n","I downloaded the text file from Project Gutenberg (http://www.gutenberg.org/cache/epub/345/pg345.txt) and placed it in my google drive. I'll give you the link in a minute.\n","\n","First, we'll load [spaCy](https://spacy.io/). This is a package with many of the same features of nltk. You can read about the differences here: https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2. The main thing we want from spacy are word-vectors, which are not available in nltk."]},{"cell_type":"code","metadata":{"id":"ZhifiwXfcAyd","colab_type":"code","colab":{}},"source":["import spacy\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"frkr8cpFRWkp","colab_type":"text"},"source":["Spacy is set up to use a GPU if available. You can turn on GPU in Runtime menu, Change runtime type. \n","\n","##However, bug for me\n","\n","When I uncomment the line below, I get an error when calling the nlp parser. I did a little digging but did not find a fix. If someone finds a way to uncomment this and avoid later errors, let me know."]},{"cell_type":"code","metadata":{"id":"kLkZvBG7P94x","colab_type":"code","colab":{}},"source":["#spacy.prefer_gpu()  #True if have GPU turned on but bug comes later\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vJslZNjSy4Xx","colab_type":"text"},"source":["Now pull in a vocabulary. You have choices here. But have to be careful. If you load the small versions, you will not get the word-vectors we want. So I am using the medium (md) version. For other choices see https://spacy.io/models/en."]},{"cell_type":"code","metadata":{"id":"Dz-yQfqTyeMU","colab_type":"code","outputId":"d6957192-af9a-4baa-a75a-ab7ec2f8b8c3","colab":{"base_uri":"https://localhost:8080/","height":261},"executionInfo":{"status":"ok","timestamp":1582751037362,"user_tz":480,"elapsed":52210,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["!python -m spacy download en_core_web_md\n"],"execution_count":72,"outputs":[{"output_type":"stream","text":["Collecting en_core_web_md==2.1.0\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n","\u001b[K     |████████████████████████████████| 95.4MB 5.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n","  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=cd964f29c3b20d3a98131be446bd9c2867b063b4b8331a84b6aac2d9d3acee62\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2f8zt6ro/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n","Successfully built en-core-web-md\n","Installing collected packages: en-core-web-md\n","Successfully installed en-core-web-md-2.1.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_md')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qD4w_Bdo-gZ1","colab_type":"code","colab":{}},"source":["import en_core_web_md\n","nlp = en_core_web_md.load()  #Gives us a way to parse text documents in one line of code. You will see in minute."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWnnKaazq-d4","colab_type":"code","colab":{}},"source":["spnlp = TypeVar('spacy.lang.en.English')  #for type hints"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ejsO0YZSJ983","colab_type":"text"},"source":["Let's do a little exploration of what we just loaded."]},{"cell_type":"code","metadata":{"id":"Lj-CntgxBEZo","colab_type":"code","outputId":"795631ad-e369-4daf-cbe8-1f06b5afafdc","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751053032,"user_tz":480,"elapsed":67861,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["nlp.vocab.length  #1.3M words"],"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1340242"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"code","metadata":{"id":"iHeO__XsR_Tw","colab_type":"code","outputId":"86143d05-f400-49ee-b2c1-e117ad627628","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751053032,"user_tz":480,"elapsed":67854,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["nlp.vocab.__contains__('marvelous')  #dunder method for checking if word in vocab"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"NgHtepfGBXwh","colab_type":"code","outputId":"13354488-00da-4894-c555-68643117c04e","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751053033,"user_tz":480,"elapsed":67850,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["nlp.vocab.__contains__('askfds')"],"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"markdown","metadata":{"id":"9RrduFcCKuOS","colab_type":"text"},"source":["##But wait, there's more\n","\n","Besides giving us a dictionary (vocab) of 1.3M words, spacy also gives us a parser which is more powerful than our current `get_clean_words`. What I want you to do is feed an entire book, as one long string, to the parser. Then we can mess around with what is produced."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9b8uv_mBBOe4"},"source":["To calculate the average color of the entire Dracula book, we'll follow these steps:\n","\n","1. Parse the book text into words using spacy's `nlp` method (actually class constructor).\n","2. Check every word to see if it names a color in our vector space, i.e., the n_colors dictionary. If it does, add it to a list of vectors.\n","3. Find the average of that list of vectors.\n","4. Find the color(s) closest to that average vector.\n","\n","The following cell performs step 1. Here is link to the txt file: https://drive.google.com/open?id=1448pTxyIcgiI94wMyB6m4bIT_nZsmFHx. You will need to add to your own google drive folder to open and read it."]},{"cell_type":"markdown","metadata":{"id":"Qd1w_kmxPycF","colab_type":"text"},"source":["##Extra credit\n","\n","I spent a little bit of time trying to find a way to directly use the url to read the file from google drive. I failed. The url is to a web page and I was getting the raw html. If someone finds a way to use the url directly, let me know for extra credit."]},{"cell_type":"code","metadata":{"id":"B4hBwLJHX3_V","colab_type":"code","colab":{}},"source":["#this can take a minute - we are parsing all of Dracula!\n","\n","import io\n","f = io.open(\"/content/gdrive/My Drive/class_tables/pg345.txt\", mode=\"r\", encoding=\"utf-8-sig\")  #if use utf-8 get \\ufeff BOM\n","\n","drac_doc = nlp(f.read())  #this is the parser doing its thing. Cool."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFHKnAuouC3d","colab_type":"code","outputId":"b8cee890-3124-4956-b1c1-e3667c9bd6fd","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751079309,"user_tz":480,"elapsed":94112,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["len(drac_doc)  #209124 tokens were parsed."],"execution_count":79,"outputs":[{"output_type":"execute_result","data":{"text/plain":["209124"]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"markdown","metadata":{"id":"6I87dbtLhqKS","colab_type":"text"},"source":["##Behind the scenes\n","\n","The nlp parser does a lot of work for us. It breaks the text into sentences and words. It tags a word with its part of speech. It even builds a parse tree and places the token within that tree. And it does Name-Entity Recognition (NER) on tokens. Bottomline: it does a lot! You can see some of it here: https://spacy.io/api/token#attributes.\n","\n","The parser returns a data structure, which I have in `drac_doc`, that holds all of this information. I don't want to get hung up on all the strange and wondrous ways you can use `drac_doc`. So I'll just give you the bare bones. "]},{"cell_type":"code","metadata":{"id":"TolPmPPOMF5r","colab_type":"code","outputId":"78cca8c8-4d12-488d-eea9-69bae2a7b610","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751079309,"user_tz":480,"elapsed":94107,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["type(drac_doc)"],"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["spacy.tokens.doc.Doc"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"id":"ZQ1Xzcf8uLMF","colab_type":"code","outputId":"85b689ef-5358-434c-bb93-3b0141ba1a28","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751079309,"user_tz":480,"elapsed":94100,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["type(drac_doc[0])  #looks like it is a sequence of tokens"],"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["spacy.tokens.token.Token"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"id":"xMpRo3wncs5t","colab_type":"code","outputId":"ffbaa1ad-2896-4625-e666-7bfd57d68428","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751079309,"user_tz":480,"elapsed":94094,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["type(drac_doc[0].text)  #and I can the word text from the token as a string"],"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["str"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"qs47AipLeCaS","colab_type":"code","outputId":"da99450a-39c2-405d-8a83-1a858047b01f","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751079310,"user_tz":480,"elapsed":94089,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["drac_doc[0].text.lower()  #might want to lower the text"],"execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'the'"]},"metadata":{"tags":[]},"execution_count":83}]},{"cell_type":"code","metadata":{"id":"76dnP__Fc8qu","colab_type":"code","outputId":"6f3daab2-bc69-4860-acca-95fe387d5821","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751079310,"user_tz":480,"elapsed":94083,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["drac_doc[0].lower_  #this does the same"],"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'the'"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"markdown","metadata":{"id":"lQdTPR_yLtKt","colab_type":"text"},"source":["##In summary\n","\n","The spacy parser gives a sequence of tokens. Each token holds lots of useful information, only a bit of which we will use for now."]},{"cell_type":"markdown","metadata":{"id":"3EEn6Q7sz5hr","colab_type":"text"},"source":["##Steps 2 and 3: find the average vector\n","\n","General strategy: go through every token in drac_doc, lower it and check if in n_colors. If it is, add its RGB vector to your matrix. When done, get the average RGB vector. Remember to use your meanv function for speed."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xG197qkQBOe5","outputId":"33cefcad-7936-429e-9221-8812403c632b","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1582751079437,"user_tz":480,"elapsed":94204,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["%%time\n","\n","# your code below\n","\n","avg_color = meanv(np.array([n_colors[i.lower_] for i in drac_doc if i.lower_ in n_colors]))"],"execution_count":85,"outputs":[{"output_type":"stream","text":["CPU times: user 198 ms, sys: 866 µs, total: 199 ms\n","Wall time: 204 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P41cMVPSy2B_","colab_type":"code","outputId":"55eaeb8c-71f0-4ed9-d7e1-1c4f8f952de0","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751079438,"user_tz":480,"elapsed":94198,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["avg_color  #array([147.44839068, 113.65371809, 100.13540511])"],"execution_count":86,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([147.44839068, 113.65371809, 100.13540511])"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n24WAxrOBOe9"},"source":["Now, we'll pass the averaged color vector to ordering function, yielding a brown mush, which is kinda what you'd expect from adding a bunch of colors together willy-nilly."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Tz-U5U8xBOe_","outputId":"96270b8d-fa2c-44b2-ee05-76d5e11e8c3f","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1582751079438,"user_tz":480,"elapsed":94192,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["dict_ordered_distances(n_colors, avg_color)[:10]"],"execution_count":87,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('reddish grey', 13.519858753013214),\n"," ('brownish grey', 15.356247186381948),\n"," ('brownish', 16.350106463486874),\n"," ('brown grey', 19.826822637698537),\n"," ('mocha', 21.824003657449868),\n"," ('grey brown', 26.730012587581818),\n"," ('puce', 28.095953180857567),\n"," ('dull brown', 28.286050911198767),\n"," ('pinkish brown', 29.719493432987974),\n"," ('dark taupe', 31.643437130672552)]"]},"metadata":{"tags":[]},"execution_count":87}]},{"cell_type":"markdown","metadata":{"id":"QCHQOcCuYZJN","colab_type":"text"},"source":["<pre>\n","[u'reddish grey',\n"," u'brownish grey',\n"," u'brownish',\n"," u'brown grey',\n"," u'mocha',\n"," u'grey brown',\n"," u'puce',\n"," u'dull brown',\n"," u'pinkish brown',\n"," u'dark taupe']\n"," </pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r14lq-GLBOfB"},"source":["On the other hand, here's what we get when we average the colors of Charlotte Perkins Gilman's classic *The Yellow Wallpaper*.  The result definitely reflects the content of the story, so maybe we're on to something here.\n","\n","Here is link to the txt file: https://drive.google.com/open?id=1lxympMbAxw_2f8CI5KPzKIS_k-zS7cQM. You will need to add to your own google drive folder to open and read it."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PPAZBXfqZDgg","colab":{}},"source":["\n","f = io.open(\"/content/gdrive/My Drive/class_tables/pg1952.txt\", mode=\"r\", encoding=\"utf-8-sig\")\n","yellow_doc = nlp(f.read())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lWj7xkIQ0nbb","colab_type":"code","outputId":"8e751635-c0d6-4aab-ff2c-d9061bf660e6","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081397,"user_tz":480,"elapsed":96139,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["len(yellow_doc)  #11649 tokens"],"execution_count":89,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11649"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"markdown","metadata":{"id":"ATc1aq630TsI","colab_type":"text"},"source":["Do the same thihg: find the average color vector for the book."]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"0e442936-65ec-4fa0-a14f-e121df582ecf","id":"XKRdubQZZDgn","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1582751081397,"user_tz":480,"elapsed":96133,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["%%time\n","\n","#your code here\n","\n","avg_color = meanv(np.array([n_colors[i.lower_] for i in yellow_doc if i.lower_ in n_colors]))"],"execution_count":90,"outputs":[{"output_type":"stream","text":["CPU times: user 14.8 ms, sys: 0 ns, total: 14.8 ms\n","Wall time: 16.3 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B2UYE5pK0cub","colab_type":"code","outputId":"0f7668a1-6c88-467c-f8d9-ae47744b28c7","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081398,"user_tz":480,"elapsed":96128,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["avg_color  #array([192.        , 185.26923077,  48.23076923])"],"execution_count":91,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([192.        , 185.26923077,  48.23076923])"]},"metadata":{"tags":[]},"execution_count":91}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3KoZyEw4BOfD","outputId":"d6585038-33dc-4cfe-db8a-4d28eb6454fa","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1582751081398,"user_tz":480,"elapsed":96122,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["dict_ordered_distances(n_colors, avg_color)[:10]"],"execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('pea', 32.867606937512456),\n"," ('puke yellow', 34.6139529618472),\n"," ('sick green', 35.25580652163731),\n"," ('vomit yellow', 37.701902232548726),\n"," ('booger', 39.06073635071867),\n"," ('olive yellow', 39.34720654280166),\n"," ('snot', 40.548768942125655),\n"," ('gross green', 41.77193998863593),\n"," ('dirty yellow', 42.05000193486195),\n"," ('mustard yellow', 42.420635957392086)]"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"markdown","metadata":{"id":"5zA1YZGaZ1Re","colab_type":"text"},"source":["<pre>\n","[u'pea',\n"," u'puke yellow',\n"," u'sick green',\n"," u'vomit yellow',\n"," u'booger',\n"," u'olive yellow',\n"," u'snot',\n"," u'gross green',\n"," u'dirty yellow',\n"," u'mustard yellow']\n"," </pre>"]},{"cell_type":"markdown","metadata":{"id":"pUzsKlkr0uLv","colab_type":"text"},"source":["Definitely captures the yellowness (in kind of a gross way!)."]},{"cell_type":"markdown","metadata":{"id":"e_V9ELtGf9WL","colab_type":"text"},"source":["#What have we learned?\n","\n","One means of converting a sequence of words into a single vector is to take the average of all the individual word vectors. We are taking the entire set of words in a book and averaging. But we can also do the same thing with smaller units, e.g., sentences, tweets. We will use that idea shortly. But next I want to return to the comat idea.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"poizyunlBOfH"},"source":["## Distributional semantics\n","\n","In the previous section, the examples are interesting because of a simple fact: colors that we think of as similar are \"closer\" to each other in RGB vector space. In our color vector space, or in our animal cuteness/size space, you can think of the words identified by vectors close to each other as being *synonyms*, in a sense: they sort of \"mean\" the same thing.  Think of this in terms of writing, say, a search engine. If someone searches for \"mauve trousers,\" then it's probably also okay to show them results for, say,"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6kpwMphfBOfJ","outputId":"c7867bae-9596-426e-c95f-74c1e00ca1ff","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1582751081398,"user_tz":480,"elapsed":96116,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["for cname, dist in dict_ordered_distances(n_colors, n_colors['mauve'])[:10]:\n","    print(cname + \" trousers\")"],"execution_count":93,"outputs":[{"output_type":"stream","text":["mauve trousers\n","dusty rose trousers\n","dusky rose trousers\n","brownish pink trousers\n","old pink trousers\n","reddish grey trousers\n","dirty pink trousers\n","old rose trousers\n","light plum trousers\n","ugly pink trousers\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kgsWNH9gBOfN"},"source":["That's all well and good for color words, which intuitively seem to exist in a multidimensional continuum of perception, and for our animal space, where we've written out the vectors ahead of time. But what about arbitrary words? Is it possible to create a vector space for all English words that has this same \"closer in space is closer in meaning\" property?\n","\n","To answer that, we have to back up a bit and ask the question: what does *meaning* mean? No one really knows, but one theory popular among computational linguists, computer scientists and other people who make search engines is the [Distributional Hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics), which states that:\n","\n","    Linguistic items with similar distributions have similar meanings.\n","    \n","What's meant by \"similar distributions\" is *similar contexts*. Take for example the following sentences:\n","\n","    It was really cold yesterday.\n","    It will be really warm today, though.\n","    It'll be really hot tomorrow!\n","    Will it be really cool Tuesday?\n","    \n","According to the Distributional Hypothesis, the words `cold`, `warm`, `hot` and `cool` must be related in some way (i.e., be close in meaning) because they occur in a similar context, i.e., between the word \"really\" and a word indicating a particular day. (Likewise, the words `yesterday`, `today`, `tomorrow` and `Tuesday` must be related, since they occur in the context of a word indicating a temperature.)\n","\n","In other words, according to the Distributional Hypothesis, a word's meaning is just a big list of all the contexts it occurs in. Two words are closer in meaning if they share contexts."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KgVJrMnQBOfQ"},"source":["## Word vectors by counting contexts\n","\n","So how do we turn this insight from the Distributional Hypothesis into a system for creating general-purpose vectors that capture the meaning of words?  Let's use a small source text to begin with, such as this excerpt from Dickens:\n","\n","    It was the best of times, it was the worst of times.\n","\n","This spreadsheet tries to capture the context of words. It is very similar to the comat of module 6.\n","\n","![dickens contexts](http://static.decontextualize.com/snaps/best-of-times.png)\n","\n","The spreadsheet has one column for every possible context, and one row for every word. The values in each cell correspond with how many times the word occurs in the given context. The numbers in the columns constitute that word's vector, i.e., the vector for the word `of` is\n","\n","    [0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n","\n","As I said, similar to the comat you built in week 6.\n","    \n","Because there are ten possible contexts, this is a ten dimensional space. You could use the same distance formula that we defined earlier to get useful information about which vectors in this space are similar to each other. In particular, the vectors for `best` and `worst` are actually the same (a distance of zero), since they occur only in the same context (`the ___ of`):\n","\n","    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n","    \n","Of course, the conventional way of thinking about \"best\" and \"worst\" is that they're *antonyms*, not *synonyms*. But they're also clearly two words of the same kind, with related meanings (through opposition), a fact that is captured by this distributional model.\n","\n","### Contexts and dimensionality\n","\n","In a corpus of any reasonable size, there will be many thousands if not many millions of possible contexts. We saw that in module 6. It turns out, though, that many of the dimensions end up being superfluous and can either be eliminated or combined with other dimensions without significantly affecting the predictive power of the resulting vectors. The process of getting rid of superfluous dimensions in a vector space is called [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction), and most implementations of count-based word-vectors make use of dimensionality reduction so that the resulting vector space has a reasonable number of dimensions (say, 100—300, depending on the corpus and application).\n","\n","The question of how to identify a \"context\" is itself very difficult to answer. In the toy example above, we've said that a \"context\" is just the word that precedes and the word that follows. Depending on your implementation of this procedure, though, you might want a context with a bigger \"window\" (e.g., two words before and after), or a non-contiguous window (skip a word before and after the given word). You might exclude certain \"function\" words like \"the\" and \"of\" when determining a word's context, or you might [lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) the words before you begin your analysis, so two occurrences with different \"forms\" of the same word count as the same context. These are all questions open to research and debate, and different implementations of procedures for creating count-based word vectors make different decisions on this issue. In module 6, we eliminated stop words but we did not go as far as lemmatizing.\n","\n","### GloVe vectors\n","\n","But you don't have to create your own word vectors from scratch! Many researchers have made downloadable databases of pre-trained vectors. One such project is Stanford's [Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove/). These 300-dimensional vectors are included with spaCy, and they're the vectors we'll be using for the rest of this module. In fact, you already have them. They come with `en_core_web_md`. Nice.\n","\n","Check this out."]},{"cell_type":"code","metadata":{"id":"igg2SCSsB5am","colab_type":"code","outputId":"89566b0e-1f00-448d-f776-1a12c8815625","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081398,"user_tz":480,"elapsed":96110,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["nlp.vocab.has_vector('frankenstein')  #check to make sure word vectors have been loaded"],"execution_count":94,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"code","metadata":{"id":"px9O-Qle-h21","colab_type":"code","colab":{}},"source":["dogv = nlp.vocab['dog'].vector  #get the 300d vector for dog"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cl7uiIyMAUed","colab_type":"code","outputId":"a75d52c5-4755-4060-8998-345ee13ee746","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081399,"user_tz":480,"elapsed":96098,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["type(dogv)"],"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"code","metadata":{"id":"lm40VLOVAX8i","colab_type":"code","outputId":"86f4dc4c-7b1d-43f1-a8e8-358eb1289199","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081399,"user_tz":480,"elapsed":96093,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["len(dogv)  #all spacy word vectors are length 300"],"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["300"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"markdown","metadata":{"id":"7OITx3mhyVUO","colab_type":"text"},"source":["You can also get a unique id for each word. This id will allow you to get a vector. So it is another way of getting the vector for a word."]},{"cell_type":"code","metadata":{"id":"1M2QLNzHQ0Vo","colab_type":"code","outputId":"f71242b7-21ef-42c3-b695-273841ef6999","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081566,"user_tz":480,"elapsed":96254,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["dog_id = nlp.vocab.strings['dog']\n","dog_id  #7562983679033046312\n"],"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7562983679033046312"]},"metadata":{"tags":[]},"execution_count":98}]},{"cell_type":"code","metadata":{"id":"A8xryirzQ8Fy","colab_type":"code","colab":{}},"source":["dogv2 = nlp.vocab.vectors[dog_id]  #another way to get a word-vector given you have word id\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RI948CDXDM9t","colab_type":"code","outputId":"3037ef8a-c84a-46fe-d1b9-4e6c43691d64","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081567,"user_tz":480,"elapsed":96244,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["all(dogv == dogv2)"],"execution_count":100,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Gxq6MTMdBOfh"},"source":["For the sake of convenience, the following function gets the vector of a given string from spaCy's vocabulary:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ACYX11K2BOfi","colab":{}},"source":["def vec(nlp:spnlp, s:str) -> narray:\n","    return nlp.vocab[s].vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6qLBNT-PIGIA","colab_type":"code","outputId":"5ec47eb3-9305-4c5a-e8b8-473c49612b48","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081568,"user_tz":480,"elapsed":96233,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["all(vec(nlp,'dog') == dogv)"],"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"code","metadata":{"id":"eKlPKKo-Tqbx","colab_type":"code","outputId":"faab6015-90ac-4bae-8a40-0ae01365acf7","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081569,"user_tz":480,"elapsed":96228,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["all(vec(nlp,'afskfsd') == 0.)  #True - a 0 vector for non-words"],"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":103}]},{"cell_type":"markdown","metadata":{"id":"q1ZUA6ONT8KU","colab_type":"text"},"source":["##Looks like a 0 vector for non-words\n","\n","Good to know."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BR4DrPOPBOfp"},"source":["The following cell shows that the cosine similarity between `dog` and `puppy` is larger than the similarity between `trousers` and `octopus`, thereby demonstrating that the vectors are working how we expect them to:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0z_UnLGmBOfq","scrolled":false,"outputId":"1dcc8c27-7c9b-4c5f-d12e-68084d3b1e1e","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081569,"user_tz":480,"elapsed":96222,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["fast_cosine(vec(nlp,'dog'), vec(nlp,'puppy')) > fast_cosine(vec(nlp,'trousers'), vec(nlp,'octopus'))"],"execution_count":104,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ftIRvBDyBOgd"},"source":["# Sentence similarity\n","\n","I am going to switch gears a bit, and move us closer to doing prediction. What I will be interested in is converting an entire sentence into a single glove vector. We can practice with the gothic authors. Here is general idea. I'll go through each row and grab the text of the sentence. I will *not* preprocess it, at least for now. Instead I will feed it directly to spacy's nlp method.\n","\n","I'll then get the vectors for all the words in the sentence (all the tokens) and take their average. I will use that resulting vector as the represnetation of the sentence.\n","\n","And as always, when I say \"I\" I mean you.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_cwe4Fz3BOge"},"source":["#Challenge 4\n","\n","Go ahead and fill out sent2vec below. s is the raw string version of a sentence.\n","\n","Use nlp to parse into tokens then get vectors of those tokens. You can see that you will get punctuation with the parse."]},{"cell_type":"code","metadata":{"id":"41tGzXcxD8Nq","colab_type":"code","colab":{}},"source":["a_doc = nlp('Now is the time.'.lower())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M4nyL02zEEeu","colab_type":"code","outputId":"0660a855-cd7f-4e19-e5a0-7c1186864e55","colab":{"base_uri":"https://localhost:8080/","height":111},"executionInfo":{"status":"ok","timestamp":1582751081570,"user_tz":480,"elapsed":96211,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["for w in a_doc:\n","  print(w)"],"execution_count":106,"outputs":[{"output_type":"stream","text":["now\n","is\n","the\n","time\n",".\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gObpbYwPGiNk","colab_type":"text"},"source":["It is interesting that spacy gives vectors to punctuation."]},{"cell_type":"code","metadata":{"id":"RfJjez1kEPLS","colab_type":"code","outputId":"9b2c2e68-acbe-4c6e-9481-05fe987ba1e7","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081570,"user_tz":480,"elapsed":96205,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["y = vec(nlp,'.')\n","len(y)  #normal 300 length vector"],"execution_count":107,"outputs":[{"output_type":"execute_result","data":{"text/plain":["300"]},"metadata":{"tags":[]},"execution_count":107}]},{"cell_type":"code","metadata":{"id":"ksv3mscnImpi","colab_type":"code","outputId":"72076939-140d-4c1a-9ebd-5dc867b7abea","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081570,"user_tz":480,"elapsed":96199,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["a_doc[4]"],"execution_count":108,"outputs":[{"output_type":"execute_result","data":{"text/plain":["."]},"metadata":{"tags":[]},"execution_count":108}]},{"cell_type":"code","metadata":{"id":"HV3ONTZkIr90","colab_type":"code","outputId":"94c93430-ef12-4115-f628-8e5cadc9a264","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081712,"user_tz":480,"elapsed":96336,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["a_doc[4].is_punct  #you can check a token by this"],"execution_count":109,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":109}]},{"cell_type":"markdown","metadata":{"id":"QQLtu8NVI_cT","colab_type":"text"},"source":["You could filter out punctuation but go ahead and use all the tokens you get from a spacy parse for now, including punctuation."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TOfTleDWBOge","colab":{}},"source":["def sent2vec(nlp:spnlp, s: str) -> narray:\n","\n","  return meanv(np.array([i.vector for i in nlp(s)]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w24_WMoQgsCj","colab_type":"text"},"source":["Test it out."]},{"cell_type":"code","metadata":{"id":"8aP_ug7egtig","colab_type":"code","colab":{}},"source":["s1vec = sent2vec(nlp, 'it was the best of times and the worst of times.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHzR0wjRhKjc","colab_type":"code","outputId":"81f377f9-c424-4e5e-bbb2-4330ee471e91","colab":{"base_uri":"https://localhost:8080/","height":92},"executionInfo":{"status":"ok","timestamp":1582751081713,"user_tz":480,"elapsed":96316,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["print(s1vec[:20])  #[ -0.07223818  0.20577    -0.06873892 -0.28904667 -0.01813886  0.01893845 ... -0.167774   -0.10611192]"],"execution_count":112,"outputs":[{"output_type":"stream","text":["[-0.07223818  0.20577    -0.06873892 -0.28904667 -0.01813886  0.01893845\n"," -0.0079125   0.00298117  0.1157698   2.459283   -0.21819901  0.10764957\n","  0.1087811  -0.10047682 -0.11054987 -0.18445301 -0.12053668  1.127795\n"," -0.167774   -0.10611192]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pHX9VKyJBOgg"},"source":["Let's find the sentence in our text file that is closest in \"meaning\" to an arbitrary input sentence. First, we'll get the list of sentences:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PJD2g-FvBOgh","colab":{}},"source":["sentences = list(drac_doc.sents)  #kind of cool. spacy keeps track of sentences for us in sents attribute"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4m8SKlZe0--X","colab_type":"code","outputId":"408ac497-df3b-4628-f2ae-1c70ebdcec3e","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081715,"user_tz":480,"elapsed":96305,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["type(sentences)"],"execution_count":114,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{"tags":[]},"execution_count":114}]},{"cell_type":"code","metadata":{"id":"m9pn9Tku2aCY","colab_type":"code","outputId":"8756d519-e333-4897-917e-e3cfd5158302","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1582751081715,"user_tz":480,"elapsed":96300,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["sentences[1]"],"execution_count":115,"outputs":[{"output_type":"execute_result","data":{"text/plain":["This eBook is for the use of anyone anywhere at no cost and with\n","almost no restrictions whatsoever.  "]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"markdown","metadata":{"id":"GX66tFCe2ZA2","colab_type":"text"},"source":["##Important note\n","\n","While it may look like sentences holds a list of strings, it does not. Instead it holds a list of spacy Spans."]},{"cell_type":"code","metadata":{"id":"gcA1SnSl1CUE","colab_type":"code","outputId":"2c6c6588-17a2-4c73-b383-7d283bc8d843","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1582751081715,"user_tz":480,"elapsed":96294,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["type(sentences[1])"],"execution_count":116,"outputs":[{"output_type":"execute_result","data":{"text/plain":["spacy.tokens.span.Span"]},"metadata":{"tags":[]},"execution_count":116}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fh9MRWWmBOgi"},"source":["The following function takes a list of sentences from a spaCy parse and compares them to an input sentence, sorting them by cosine similarity.\n","\n","I'm going to give you the code to avoid you having to delve into the innards of spacy Spans. I did the delving for you :) But note that `[w.vector for w in x]` asks for each token w in span x and then grabs the vector of w."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H3sNbiVrBOgj","colab":{}},"source":["def spacy_closest_sent(nlp:spnlp, space:list, input_str:str, n:int=10):\n","  assert isinstance(space, list)\n","  assert all([isinstance(sp, spacy.tokens.span.Span) for sp in space])\n","\n","  input_vec = sent2vec(nlp, input_str)\n","  return sorted(space,\n","                key=lambda x: fast_cosine(np.mean([w.vector for w in x], axis=0), input_vec),\n","                reverse=True)[:n]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"H-fj5w6mBOgn"},"source":["Here are the sentences in *Dracula* closest in meaning to \"My favorite food is strawberry ice cream.\" (Extra linebreaks are present because we didn't strip them out when we originally read in the source text.)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eHRwZ6qlBOgo","outputId":"3441412e-8951-483d-e793-1c52782044d2","colab":{"base_uri":"https://localhost:8080/","height":727},"executionInfo":{"status":"ok","timestamp":1582751083991,"user_tz":480,"elapsed":98558,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}}},"source":["for sent in spacy_closest_sent(nlp, sentences, \"My favorite food is strawberry ice cream.\", 10):\n","    print( sent.text)\n","    print( \"---\")"],"execution_count":118,"outputs":[{"output_type":"stream","text":["This, with some cheese\n","and a salad and a bottle of old Tokay, of which I had two glasses, was\n","my supper.\n","---\n","I set to and\n","enjoyed a hearty meal.\n","---\n","There was everywhere a bewildering mass of fruit blossom--apple,\n","plum, pear, cherry; and as we drove by I could see the green grass under\n","\n","---\n","I had for breakfast more paprika, and a sort of porridge of maize flour\n","which they said was \"mamaliga,\" and egg-plant stuffed with forcemeat, a\n","very excellent dish, which they call \"impletata.\n","---\n","I had for dinner, or\n","rather supper, a chicken done up some way with red pepper, which was\n","very good but thirsty.\n","---\n","I dined on what they\n","called \"robber steak\"--bits of bacon, onion, and beef, seasoned with red\n","pepper, and strung on sticks and roasted over the fire, in the simple\n","style of the London cat's meat!\n","---\n","We get hot soup, or coffee, or tea; and\n","off we go.\n","---\n","I got a cup of tea at the Aërated Bread Company\n","and came down to Purfleet by the next train.\n","\n","\n","---\n","There is not even a toilet glass on my\n","table, and I had to get the little shaving glass from my bag before I\n","could either shave or brush my hair.\n","---\n","Come, and we'll have a cup of tea somewhere.\n","---\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nxnRcVYLUCFO","colab_type":"text"},"source":["#Congrats\n","\n","You are done with part A. The important point now is to collect your functions into your library. You will need them in part B.\n","\n","My strategy was to create a new file in my repository to hold the functions I defined in this notebook. It is only these that we will need going forward. So decided to separate them out. You can load just the single file as so:\n","<pre>\n","from steve_rep.scacy_fns import *\n","</pre>\n","where `spacy_fns.py` is the file I created under repository `steve_rep`. When you are done with that, move on to part B."]}]}