{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"w20_week3_bayes_handout.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"m3DzKIjOtezZ","colab_type":"text"},"source":["<h1>\n","<center>\n","Module 3 - Naive Bayes\n","</center>\n","</h1>\n","<div class=h1_cell>\n","<p>\n","\n","We will look at a competitor to K-NN this week, Naive Bayes. We will continue to work with the tweets so let's bring them in.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"nGvpBPzqhbq6","colab_type":"text"},"source":["##Use a url\n","\n","You can see I am using a url below. Reminder of what to do to get it.\n","\n","1. I went into Google Drive and found the file `tweets_shuffled.csv`. This is the file I wrote out at end of module 2.\n","2. Right click and then open in Sheets.\n","3. Publish to web from File menu.\n","4. Choose csv as the type. Click publish.\n","5. Copy the url you are given and paste it in below.\n","\n","Pretty slick. We can now read the file from anywhere with just the url. No need to mount anything."]},{"cell_type":"code","metadata":{"id":"nj8czM2L3G2f","colab_type":"code","colab":{}},"source":["import pandas as pd\n","url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vS1Wbl-BAGw9WxwHU4IA6NSwH8hYF7nKs906aTqsOpV_Vy82R5_shV2W1y0gQVk3cGQJHzCNdGibcPS/pub?output=csv'\n","tweet_table = pd.read_csv(url)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"oNG1VPG1tezp","colab_type":"code","outputId":"7afe593b-06b6-4786-d840-c41c6f7d6079","executionInfo":{"status":"ok","timestamp":1579746836859,"user_tz":480,"elapsed":1783,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["tweet_table.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","      <th>length</th>\n","      <th>#</th>\n","      <th>!</th>\n","      <th>a</th>\n","      <th>b</th>\n","      <th>c</th>\n","      <th>d</th>\n","      <th>e</th>\n","      <th>f</th>\n","      <th>g</th>\n","      <th>h</th>\n","      <th>i</th>\n","      <th>j</th>\n","      <th>k</th>\n","      <th>l</th>\n","      <th>m</th>\n","      <th>n</th>\n","      <th>o</th>\n","      <th>p</th>\n","      <th>q</th>\n","      <th>r</th>\n","      <th>s</th>\n","      <th>t</th>\n","      <th>u</th>\n","      <th>v</th>\n","      <th>w</th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>z</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>happy weekend! ð#quoteoftheday #quote #mind...</td>\n","      <td>71</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>i have 1 homegirl ðð</td>\n","      <td>26</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@user pray for my country. 2days flooding.   g...</td>\n","      <td>96</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>can't believe i was up through the 3hrs of #or...</td>\n","      <td>136</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>10</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>11</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>great way to finish a great week off. #workupa...</td>\n","      <td>102</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>8</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                              tweet  length  ...  x  y  z\n","0      0  happy weekend! ð#quoteoftheday #quote #mind...      71  ...  0  2  0\n","1      0                         i have 1 homegirl ðð      26  ...  0  0  0\n","2      0  @user pray for my country. 2days flooding.   g...      96  ...  0  4  0\n","3      0  can't believe i was up through the 3hrs of #or...     136  ...  0  1  0\n","4      0  great way to finish a great week off. #workupa...     102  ...  0  3  0\n","\n","[5 rows x 31 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"j_GdU8ACeitW","colab_type":"code","colab":{}},"source":["#flush the old directory\n","!rm -r  'w20_ds_library'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rPISZH5nclhU","colab_type":"code","colab":{}},"source":["my_github_name = 'FutureDeus'  #replace with your account name"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NuaxZXKXcrNL","colab_type":"code","colab":{}},"source":["clone_url = f'https://github.com/{my_github_name}/w20_ds_library.git'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"ff7abb22-5ed9-456a-b6bb-16941bf6c6e0","id":"Rmp86ySdkr4z","executionInfo":{"status":"ok","timestamp":1579746841635,"user_tz":480,"elapsed":6535,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["#get the latest.\n","!git clone $clone_url \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'w20_ds_library'...\n","remote: Enumerating objects: 26, done.\u001b[K\n","remote: Counting objects:   3% (1/26)\u001b[K\rremote: Counting objects:   7% (2/26)\u001b[K\rremote: Counting objects:  11% (3/26)\u001b[K\rremote: Counting objects:  15% (4/26)\u001b[K\rremote: Counting objects:  19% (5/26)\u001b[K\rremote: Counting objects:  23% (6/26)\u001b[K\rremote: Counting objects:  26% (7/26)\u001b[K\rremote: Counting objects:  30% (8/26)\u001b[K\rremote: Counting objects:  34% (9/26)\u001b[K\rremote: Counting objects:  38% (10/26)\u001b[K\rremote: Counting objects:  42% (11/26)\u001b[K\rremote: Counting objects:  46% (12/26)\u001b[K\rremote: Counting objects:  50% (13/26)\u001b[K\rremote: Counting objects:  53% (14/26)\u001b[K\rremote: Counting objects:  57% (15/26)\u001b[K\rremote: Counting objects:  61% (16/26)\u001b[K\rremote: Counting objects:  65% (17/26)\u001b[K\rremote: Counting objects:  69% (18/26)\u001b[K\rremote: Counting objects:  73% (19/26)\u001b[K\rremote: Counting objects:  76% (20/26)\u001b[K\rremote: Counting objects:  80% (21/26)\u001b[K\rremote: Counting objects:  84% (22/26)\u001b[K\rremote: Counting objects:  88% (23/26)\u001b[K\rremote: Counting objects:  92% (24/26)\u001b[K\rremote: Counting objects:  96% (25/26)\u001b[K\rremote: Counting objects: 100% (26/26)\u001b[K\rremote: Counting objects: 100% (26/26), done.\u001b[K\n","remote: Compressing objects:   4% (1/25)\u001b[K\rremote: Compressing objects:   8% (2/25)\u001b[K\rremote: Compressing objects:  12% (3/25)\u001b[K\rremote: Compressing objects:  16% (4/25)\u001b[K\rremote: Compressing objects:  20% (5/25)\u001b[K\rremote: Compressing objects:  24% (6/25)\u001b[K\rremote: Compressing objects:  28% (7/25)\u001b[K\rremote: Compressing objects:  32% (8/25)\u001b[K\rremote: Compressing objects:  36% (9/25)\u001b[K\rremote: Compressing objects:  40% (10/25)\u001b[K\rremote: Compressing objects:  44% (11/25)\u001b[K\rremote: Compressing objects:  48% (12/25)\u001b[K\rremote: Compressing objects:  52% (13/25)\u001b[K\rremote: Compressing objects:  56% (14/25)\u001b[K\rremote: Compressing objects:  60% (15/25)\u001b[K\rremote: Compressing objects:  64% (16/25)\u001b[K\rremote: Compressing objects:  68% (17/25)\u001b[K\rremote: Compressing objects:  72% (18/25)\u001b[K\rremote: Compressing objects:  76% (19/25)\u001b[K\rremote: Compressing objects:  80% (20/25)\u001b[K\rremote: Compressing objects:  84% (21/25)\u001b[K\rremote: Compressing objects:  88% (22/25)\u001b[K\rremote: Compressing objects:  92% (23/25)\u001b[K\rremote: Compressing objects:  96% (24/25)\u001b[K\rremote: Compressing objects: 100% (25/25)\u001b[K\rremote: Compressing objects: 100% (25/25), done.\u001b[K\n","remote: Total 26 (delta 14), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects:   3% (1/26)   \rUnpacking objects:   7% (2/26)   \rUnpacking objects:  11% (3/26)   \rUnpacking objects:  15% (4/26)   \rUnpacking objects:  19% (5/26)   \rUnpacking objects:  23% (6/26)   \rUnpacking objects:  26% (7/26)   \rUnpacking objects:  30% (8/26)   \rUnpacking objects:  34% (9/26)   \rUnpacking objects:  38% (10/26)   \rUnpacking objects:  42% (11/26)   \rUnpacking objects:  46% (12/26)   \rUnpacking objects:  50% (13/26)   \rUnpacking objects:  53% (14/26)   \rUnpacking objects:  57% (15/26)   \rUnpacking objects:  61% (16/26)   \rUnpacking objects:  65% (17/26)   \rUnpacking objects:  69% (18/26)   \rUnpacking objects:  73% (19/26)   \rUnpacking objects:  76% (20/26)   \rUnpacking objects:  80% (21/26)   \rUnpacking objects:  84% (22/26)   \rUnpacking objects:  88% (23/26)   \rUnpacking objects:  92% (24/26)   \rUnpacking objects:  96% (25/26)   \rUnpacking objects: 100% (26/26)   \rUnpacking objects: 100% (26/26), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9r8-rTeMkr5C","colab":{}},"source":["from w20_ds_library import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5D6weatwpF6E","colab_type":"code","outputId":"82520c86-aee1-421a-c1d7-70a7b13007e2","executionInfo":{"status":"ok","timestamp":1579746841853,"user_tz":480,"elapsed":6740,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":409}},"source":["%whos"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Variable                    Type            Data/Info\n","-----------------------------------------------------\n","Callable                    CallableMeta    typing.Callable\n","TypeVar                     TypingMeta      typing.TypeVar\n","char_set                    str             #!abcdefghijklmnopqrstuvwxyz\n","clone_url                   str             https://github.com/FutureDeus/w20_ds_library.git\n","cm_accuracy                 function        <function cm_accuracy at 0x7f5aa39296a8>\n","cosine_similarity           function        <function cosine_similarity at 0x7f5aa3929730>\n","dframe                      TypeVar         ~pd.core.frame.DataFrame\n","hello_ds                    function        <function hello_ds at 0x7f5aa3929400>\n","inverse_cosine_similarity   function        <function inverse_cosine_<...>larity at 0x7f5aa39297b8>\n","knn                         function        <function knn at 0x7f5aa3929598>\n","knn_tester                  function        <function knn_tester at 0x7f5aa3929620>\n","my_github_name              str             FutureDeus\n","np                          module          <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n","ordered_distances           function        <function ordered_distances at 0x7f5aa3929510>\n","pd                          module          <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n","sortSecond                  function        <function sortSecond at 0x7f5aa3929488>\n","tweet_table                 DataFrame              label             <...>[29509 rows x 31 columns]\n","url                         str             https://docs.google.com/s<...>zCNdGibcPS/pub?output=csv\n","w20_ds_library              module          <module 'w20_ds_library.w<...>brary/w20_ds_library.py'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-J0slaJrtezw","colab_type":"text"},"source":["<h2>\n","Another method: Naive Bayes\n","</h2>\n","\n","The problem is still the same. Given a tweet, predict whether it is hate or nohate. The twist is that I am going to reform the question in terms of probabilities using a method called *Naive Bayes*. I'll tell you what is naive about it later. Fow now, you need to do a perspective shift. We will no longer be using a table-based approach that looks at columns of features. Instead we will use counts of things. First, here is the formula we want to compute.\n","\n","<img src='https://www.dropbox.com/s/gstzvvtvh9b39o8/bayes.png?raw=1'>\n","\n"," This formula applies for any tweet we are looking at. It produces a prediction for that tweet.\n","We read the left hand side as \"The probability of value O (hate or nohate) given the evidence which we supply from the tweet. In later modules we will be looking at words as evidence. In this module, to keep it simple, we will look at the letters as evidence.\n","More specifically, on the left hand side looking at P(O|...), the O stands for one of the prediction values. In our case 0 (nohate) or 1 (hate).\n","On the right hand side of the bar, E1 ... En stands for the evidence we have for making prediction O, i.e., the *unique* letters that show up in the tweet.  So E1 could be 'a', E2 could be 'c', etc. This means we see an 'a' and a 'c' and other letters in the tweet. The way we will use this formula is as follows:\n","<p>\n","<ol>\n","<p>\n","<li>We have a tweet T made up of a set of letters L. We want to know whether to mark it as hate (1) or nohate (0).</li>\n","<p>\n","<li>We compute the left hand side mulitple times, one for each value of O. So in our case we first compute P(nohate|L). What is the probability that T is nohate given its set of letters L. \n","<p><li>We then compute P(hate|L). What is the probability that T is hate given its set of letters L.</li>\n","<p><li>We compare the 2 probabilities. The higher probability wins and we use the associated value for prediction.</li>\n","</ol>\n","<p>\n","In summary, we always compute the probability for all possible prediction values. We only have two, hate and nohate, so we always compute two. If you had three possible values (next week), you would compute three probabilities, etc.\n","  <p>\n","Ok, what about the right hand side? We can assume that O has been assigned to one of the classes, nohate or hate, on the left hand side. For sake of discussion, let's say O is assigned to hate. So we are trying to compute P(hate|...).\n","<ul>\n","<li>Let's look at the numerator first. We see a sequence of multiplications with terms that look like P(Ei|hate) - I filled in the value for O that we chose. Let's assume E1 is evidence that 'a' is in the set L. Then P('a'|hate) stands for the probability we see the letter 'a' associated with the case hate. This ends up being the total number of times we see 'a' with hate (across all tweets in the training set) divided by the total number of hate tweets (in the training set). If the letter 'a' appears in every hate tweet, then P('a'|hate) = 1.\n","If it never appears in a hate tweet, then P('a'|hate) is 0. Typically, it is somewhere inbetween.\n","<li>So we compute the numerator by doing a bunch of lookups for each letter in L in terms of hate. And multiplying together. That is the product of all those Ei terms.</li>\n","<p><li>What about P(O) at the end? In our example, P(hate). This is just the percentage of tweets that are case hate. Note we can compute P(hate) and P(nohate) just once; they do not change with the tweet. They are global values.</li>\n","<p><li>We are going to ignore the denominator! Wait. You can't just throw out the denominator. But we can. Do you see why? All we care about is the relative difference between the probabilities for P(hate) and P(nohate). Dividing by a constant does nothing to change that relative difference. If one is greater than another, then dividing both by a constant does not change that. So we throw out the denominator.\n","</ul>\n","\n","Ok, so have to get lots of counts of things and do some dividing to get probabilities. I could use a pandas table again to organize our counts. But there is a much faster way: use a dictionary. I'll show you a start and then ask you to complete.\n"]},{"cell_type":"markdown","metadata":{"id":"WozN4Anuo7yL","colab_type":"text"},"source":["##Get first tweet and create set L"]},{"cell_type":"code","metadata":{"id":"IgFXJclAo_zM","colab_type":"code","colab":{}},"source":["tweet0 = tweet_table.loc[0, 'tweet']\n","label0 = tweet_table.loc[0, 'label']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CSVmFxVbqxAu","colab_type":"code","outputId":"fce002ce-930b-4760-8bd7-a6568a99fc01","executionInfo":{"status":"ok","timestamp":1579746841854,"user_tz":480,"elapsed":6727,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["label0"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"8muFFdxspLyI","colab_type":"code","outputId":"e479a6a5-8025-40ff-8231-4326ee14a80a","executionInfo":{"status":"ok","timestamp":1579746841854,"user_tz":480,"elapsed":6719,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":447}},"source":["L0 = set(tweet0).intersection(set('abcdefghijklmnopqrstuvwxyz!#'))\n","L0"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'!',\n"," '#',\n"," 'a',\n"," 'b',\n"," 'd',\n"," 'e',\n"," 'f',\n"," 'h',\n"," 'i',\n"," 'j',\n"," 'k',\n"," 'l',\n"," 'm',\n"," 'n',\n"," 'o',\n"," 'p',\n"," 'q',\n"," 'r',\n"," 's',\n"," 't',\n"," 'u',\n"," 'w',\n"," 'y'}"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"5pwAlxrnO3Rs","colab_type":"text"},"source":["Notice I am ignoring how many times a letter appears in a tweet. Just interested in if it appears. L0 has the letters that appear but not how many times."]},{"cell_type":"markdown","metadata":{"id":"KDxvZ6JLqIC1","colab_type":"text"},"source":["# Challenge 1\n","\n","Build a dictionary of following form:\n","<pre>\n","{a_letter: [nohate_count, hate_count], ...}\n","</pre>\n","\n","So go through each character in L0 and add it to dictionary with a value of (1,0). Why (1,0)? Because it is a nohate tweet so we add 1 to nohate position.  Here are my results:\n","<pre>\n","{'!': [1, 0],\n"," '#': [1, 0],\n"," 'a': [1, 0],\n"," 'b': [1, 0],\n"," 'd': [1, 0],\n"," 'e': [1, 0],\n"," 'f': [1, 0],\n"," 'h': [1, 0],\n"," 'i': [1, 0],\n"," 'j': [1, 0],\n"," 'k': [1, 0],\n"," 'l': [1, 0],\n"," 'm': [1, 0],\n"," 'n': [1, 0],\n"," 'o': [1, 0],\n"," 'p': [1, 0],\n"," 'q': [1, 0],\n"," 'r': [1, 0],\n"," 's': [1, 0],\n"," 't': [1, 0],\n"," 'u': [1, 0],\n"," 'w': [1, 0],\n"," 'y': [1, 0]}\n","</pre>"]},{"cell_type":"code","metadata":{"id":"zUKCgTCjrGEc","colab_type":"code","colab":{}},"source":["#your code here\n","char_bag = {key: [1, 0] for key in L0}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_8EA6k2rUQo","colab_type":"code","outputId":"8a70187e-cdb8-4c55-edc2-0e09c376ae62","executionInfo":{"status":"ok","timestamp":1579746841855,"user_tz":480,"elapsed":6702,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":447}},"source":["char_bag"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'!': [1, 0],\n"," '#': [1, 0],\n"," 'a': [1, 0],\n"," 'b': [1, 0],\n"," 'd': [1, 0],\n"," 'e': [1, 0],\n"," 'f': [1, 0],\n"," 'h': [1, 0],\n"," 'i': [1, 0],\n"," 'j': [1, 0],\n"," 'k': [1, 0],\n"," 'l': [1, 0],\n"," 'm': [1, 0],\n"," 'n': [1, 0],\n"," 'o': [1, 0],\n"," 'p': [1, 0],\n"," 'q': [1, 0],\n"," 'r': [1, 0],\n"," 's': [1, 0],\n"," 't': [1, 0],\n"," 'u': [1, 0],\n"," 'w': [1, 0],\n"," 'y': [1, 0]}"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"caIj3jF5rh8w","colab_type":"text"},"source":["# Challenge 2\n","\n","Ok, the harder part. I want char_bag to reflect the char counts for all the tweets in a table. I'm going to do a more traditional split of tweet_table into a training_table and a testing_table. Remember the table got randomly shuffled last week so this is a random selection of testing and training data."]},{"cell_type":"code","metadata":{"id":"OGcJTOm-sdfx","colab_type":"code","outputId":"50c851a6-8495-4f22-a437-fcd96ee1bb13","executionInfo":{"status":"ok","timestamp":1579746841856,"user_tz":480,"elapsed":6695,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["len(tweet_table)/3  #I am carving off 33% to act as testing, rest is training"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9836.333333333334"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"AtGhodqDsY7s","colab_type":"code","colab":{}},"source":["testing_table = tweet_table[:9836]  #use 1/3 to test\n","training_table = tweet_table[9836:]  #use 2/3 to train"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QajHUTaHs37d","colab_type":"text"},"source":["Go ahead and go through every tweet in training_table, compute L, then add counts to appropriate value in pair. My results:\n","<pre>\n","{'!': [4469, 214],\n"," '#': [13176, 956],\n"," 'a': [17757, 1319],\n"," 'b': [11084, 902],\n"," 'c': [12735, 1126],\n"," 'd': [15337, 1121],\n"," 'e': [18088, 1336],\n"," 'f': [12660, 881],\n"," 'g': [13357, 987],\n"," 'h': [15808, 1218],\n"," 'i': [17429, 1312],\n"," 'j': [2585, 241],\n"," 'k': [8812, 663],\n"," 'l': [15773, 1216],\n"," 'm': [14187, 1136],\n"," 'n': [17087, 1287],\n"," 'o': [17550, 1312],\n"," 'p': [11848, 998],\n"," 'q': [747, 68],\n"," 'r': [17356, 1304],\n"," 's': [17670, 1327],\n"," 't': [17430, 1308],\n"," 'u': [15675, 1238],\n"," 'v': [9061, 624],\n"," 'w': [11912, 939],\n"," 'x': [2104, 193],\n"," 'y': [14627, 1001],\n"," 'z': [1255, 105]}\n"," </pre>"]},{"cell_type":"code","metadata":{"id":"0oQKLDIytfWU","colab_type":"code","outputId":"35c47f52-6f07-45d6-8d28-29072b4f35f7","executionInfo":{"status":"ok","timestamp":1579746844366,"user_tz":480,"elapsed":9192,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["%%time\n","char_bag = {}\n","\n","#your code here\n","char_bag = {key: list([0, 0]) for key in set(char_set)}\n","for index, row in training_table.iterrows():\n","  flag = row['label']\n","  tweet_temp = set(row['tweet']).intersection(set(char_set))\n","  for i in tweet_temp:\n","    char_bag[i][flag] += 1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 2.5 s, sys: 8.6 ms, total: 2.5 s\n","Wall time: 2.52 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H5BWGoi8vFUY","colab_type":"code","outputId":"5d31fd63-655d-460c-b312-2f7028de5613","executionInfo":{"status":"ok","timestamp":1579746844367,"user_tz":480,"elapsed":9184,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":540}},"source":["char_bag"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'!': [4469, 214],\n"," '#': [13176, 956],\n"," 'a': [17757, 1319],\n"," 'b': [11084, 902],\n"," 'c': [12735, 1126],\n"," 'd': [15337, 1121],\n"," 'e': [18088, 1336],\n"," 'f': [12660, 881],\n"," 'g': [13357, 987],\n"," 'h': [15808, 1218],\n"," 'i': [17429, 1312],\n"," 'j': [2585, 241],\n"," 'k': [8812, 663],\n"," 'l': [15773, 1216],\n"," 'm': [14187, 1136],\n"," 'n': [17087, 1287],\n"," 'o': [17550, 1312],\n"," 'p': [11848, 998],\n"," 'q': [747, 68],\n"," 'r': [17356, 1304],\n"," 's': [17670, 1327],\n"," 't': [17430, 1308],\n"," 'u': [15675, 1238],\n"," 'v': [9061, 624],\n"," 'w': [11912, 939],\n"," 'x': [2104, 193],\n"," 'y': [14627, 1001],\n"," 'z': [1255, 105]}"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"HOpA0K7AvsaH","colab_type":"text"},"source":["###Have most of what we need\n","\n","Check it out. If I want to compute something like P('w'|hate), then I look up the value of 'w' which is [11912, 939]. Because it is hate, I pull out 939. That is the numerator. The denominator is the total number of hate tweets in training_table. Let's get that now."]},{"cell_type":"code","metadata":{"id":"KqGz-BTPwhD8","colab_type":"code","colab":{}},"source":["label_list = training_table['label'].tolist()  #convert column into a list of values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PP6CC7pSwrEQ","colab_type":"code","outputId":"e6762b19-dd7a-44ef-9761-066412dd2c36","executionInfo":{"status":"ok","timestamp":1579746844368,"user_tz":480,"elapsed":9171,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["hate_count = label_list.count(1)  #only need to compute this once - it does not change\n","hate_count"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1345"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"048e9afd-88fa-4697-dfc0-eb67e5526ef6","id":"dgMvF5h7w3IN","executionInfo":{"status":"ok","timestamp":1579746844368,"user_tz":480,"elapsed":9163,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["nohate_count = label_list.count(0)    #only need to compute this once - it does not change\n","nohate_count"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["18328"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"ijrwG0qgw813","colab_type":"text"},"source":["So P('w'|hate) = 939/1345."]},{"cell_type":"code","metadata":{"id":"fedf8m86xNbz","colab_type":"code","outputId":"4e1181a0-c8ff-4da0-ee21-06f8689e65cc","executionInfo":{"status":"ok","timestamp":1579746844369,"user_tz":480,"elapsed":9156,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["p_w_hate = 939/1345\n","p_w_hate"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6981412639405205"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"re8ma3ysxkrT","colab_type":"text"},"source":["Cool. We have a way to compute P(Ei|O). We are still missing P(O). Easy peasy."]},{"cell_type":"code","metadata":{"id":"wF1PPFQpx0q8","colab_type":"code","outputId":"2b16e246-741f-410b-bb32-183ffe7d6ee7","executionInfo":{"status":"ok","timestamp":1579746844369,"user_tz":480,"elapsed":9148,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["p_hate = hate_count/len(training_table)\n","p_hate"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0683678137548925"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"d33136ea-b1ba-4632-91b0-6b49c22a8a57","id":"mHYbXmwUx9Xb","executionInfo":{"status":"ok","timestamp":1579746844369,"user_tz":480,"elapsed":9141,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["p_nohate = nohate_count/len(training_table)\n","p_nohate"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9316321862451075"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"qeNxKrUVyIvW","colab_type":"text"},"source":["# Challenge 3\n","\n","Now for testing. Let's start simple. Give me the probability of P(hate|LT0) and P(nohate|LT0) where LT0 is the char set in the first tweet of the testing_table (not the training_table)."]},{"cell_type":"code","metadata":{"id":"Q8dInR_3yq56","colab_type":"code","colab":{}},"source":["tweet0 = testing_table.loc[0,'tweet']\n","label0 = testing_table.loc[0,'label']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OqFomJ5X1TDm","colab_type":"code","outputId":"b4d104b2-763d-4345-d605-e2586de7a6f3","executionInfo":{"status":"ok","timestamp":1579746844514,"user_tz":480,"elapsed":9267,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["tweet0"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'happy weekend! ð\\x9f\\x93\\x8d#quoteoftheday #quote #mindfulness   #summer #justbe'"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"lWXgBjSb1VYC","colab_type":"code","outputId":"cdb6708e-2138-4773-a8b4-dec282d8a3c3","executionInfo":{"status":"ok","timestamp":1579746844515,"user_tz":480,"elapsed":9259,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["label0"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"869b8a63-4a45-465c-a6fb-a92c82b6a021","id":"IcJHDacwy8B1","executionInfo":{"status":"ok","timestamp":1579746844515,"user_tz":480,"elapsed":9249,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":447}},"source":["LT0 = set(tweet0).intersection(char_set)\n","LT0"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'!',\n"," '#',\n"," 'a',\n"," 'b',\n"," 'd',\n"," 'e',\n"," 'f',\n"," 'h',\n"," 'i',\n"," 'j',\n"," 'k',\n"," 'l',\n"," 'm',\n"," 'n',\n"," 'o',\n"," 'p',\n"," 'q',\n"," 'r',\n"," 's',\n"," 't',\n"," 'u',\n"," 'w',\n"," 'y'}"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"UXtQNaqGzU2C","colab_type":"code","colab":{}},"source":["#your code here to compute nohate probability\n","\n","no_numerator = np.multiply(np.prod([np.divide(char_bag[key][0], nohate_count) for key in LT0]), p_nohate)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2FH_yWk0iZS","colab_type":"code","outputId":"1f0e6c51-8c36-4738-ae4d-cfd25e026cbf","executionInfo":{"status":"ok","timestamp":1579746844516,"user_tz":480,"elapsed":9236,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["no_numerator  #1.832109487333443e-05"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.832109487333442e-05"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"68rQjd3L0RUJ","colab":{}},"source":["#your code here to compute hate probability\n","\n","h_numerator = np.multiply(np.prod([np.divide(char_bag[key][1], hate_count) for key in LT0]), p_hate)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y1AdZnGE0n9B","colab_type":"code","outputId":"ce27a8d1-fdbe-4f8e-9761-090df2421c71","executionInfo":{"status":"ok","timestamp":1579746844517,"user_tz":480,"elapsed":9222,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["h_numerator  #2.5591876996227343e-06"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.5591876996227335e-06"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"DSAYCAwM08zi","colab_type":"text"},"source":["##Nohate wins!\n","\n","The nohate probability is larger (or less small from another perspective). So we would predict nohate or 0 for this tweet. And it is in fact labeled as a nohate tweet so we chalk up a correct prediction. Go Bayes!"]},{"cell_type":"markdown","metadata":{"id":"NOwPBNFvbxVd","colab_type":"text"},"source":["# Challenge 4\n","\n","What we need now is the equivalent of knn from module 1. As a reminder, here is its signature line:\n","<pre>\n","def knn(target_vector:list, crowd_table:dframe, answer_column:str, k:int, dfunc:Callable) -> int:\n","</pre>\n","\n","The signature line needs to change. Here is what I propose:\n","<pre>\n","def bayes(evidence:set, evidence_bag:dict, training_table:dframe) -> tuple:\n","</pre>\n","where\n","* evidence is the set of chars we saw in the tweet, e.g., LT0.\n","* evidence_bag is the dictionary we built from the training table, e.g., char_bag.\n","* training_table is the training table. You can assume that the training_table has a label column and that it contains integer labels, e.g., 0, 1, etc.\n","\n","Given just these values, I believe we can compute what we need for Naive Bayes.\n","\n","Note that I have the function returning a tuple. What I propose is for the function to return the probabilities for all the classes and let someone outside figure out which is the largest. I am doing this because I don't want to lose the probabilities. We may be able to use them in later weeks to shift the outcome in directions that we want, e.g., to overcome bias in our training data.\n","\n","For testing, try it on LT0 and make sure you get same results as above."]},{"cell_type":"markdown","metadata":{"id":"WTwC9hO0kh_g","colab_type":"text"},"source":["## Important note 1\n","\n","I would not worry about efficiency here. Write the most understandable algorithm you can. We can worry about making it more efficient later.\n","\n","In particular, given that you are only passing in the training_table, you will have to compute number of classes, class counts and class probabilities from it. Rather not rely on globals."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MKpWHThEk5pD"},"source":["## Important note 2\n","\n","Do not hard-wire your function to the hate-nohate 2-class example. Next week we will be looking at a new problem that has 3 classes. I expect your function to spit out a tuple with 3 probabilities in that case.\n"]},{"cell_type":"code","metadata":{"id":"UgkoyBdYfcs-","colab_type":"code","colab":{}},"source":["def bayes(evidence:set, evidence_bag:dict, training_table:dframe) -> tuple:\n","  assert isinstance(evidence, set), f'evidence not a set but instead a {type(evidence)}'\n","  assert isinstance(evidence_bag, dict), f'evidence_bag not a dict but instead a {type(evidence_bag)}'\n","  assert isinstance(training_table, pd.core.frame.DataFrame), f'training_table not a dataframe but instead a {type(training_table)}'\n","  assert 'label' in training_table, f'label column is not found in training_table'\n","  assert training_table.label.dtype == int, f\"label must be an int column (possibly wrangled); instead it has type({training_table.label.dtype})\"\n","\n","  #your code here\n","  label_list = training_table['label'].tolist()\n","\n","  variation = max(label_list)+1\n","  \n","  count = [label_list.count(i) for i in range(variation)]\n","  p_count = [j/len(training_table) for j in count]\n","\n","  return tuple(np.multiply(np.prod([np.divide(evidence_bag[key][i], count[i]) for key in evidence]), p_count[i]) for i in range(variation))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMkzxR6tiv4R","colab_type":"code","outputId":"d0e9e74c-7885-436a-d0b6-03075db850fa","executionInfo":{"status":"ok","timestamp":1579746844517,"user_tz":480,"elapsed":9213,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["bayes(LT0, char_bag, training_table)  #(1.832109487333443e-05, 2.5591876996227335e-06)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1.832109487333442e-05, 2.5591876996227335e-06)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mWZF2YJ8crR4"},"source":["# Challenge 5\n","\n","We are now ready for bayes_tester. I'll give you the start of the function below. I would like it to be a little different than knn_tester, which produces a dictionary of the 4 cases. Have bayes_tester return a list of the tuples you get back from bayes. We can mess with the 4 cases a little later.\n","\n","<pre>\n","def bayes_tester(testing_table:dframe, evidence_bag:dict, training_table:dframe, parser:Callable) -> list:\n","</pre>\n","\n","You noticed I added a parser argument? I am doing this to make our tester a bit more general. This week we are pulling chars out of text. Next week we will be pulling words out of text. The parser function is used to transform the text into the form you want. I'll give you the parser for this week below."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kJCoBO9bv0kv","colab":{}},"source":["def char_set_builder(text:str) -> list:\n","  the28 = set(text).intersection(set('abcdefghijklmnopqrstuvwxyz!#'))\n","  return list(the28)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9l178SUW9bku","colab_type":"text"},"source":["One more thing. I am going to have the function assume that the text is under a column called 'text'. That means we have to re-wrangle."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2HowdHEJ9ogK","colab":{}},"source":["bayes_testing_table = testing_table.rename(columns={\"tweet\": \"text\"})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YcRuTnLO9L9n","colab_type":"text"},"source":["Ok, I think you are good to go. Define knn_tester and then test it as follows:\n","<pre>\n","%%time\n","prob_list = bayes_tester(bayes_testing_table, char_bag, training_table, char_set_builder)\n","</pre>"]},{"cell_type":"code","metadata":{"id":"o8UTCltEtcZ5","colab_type":"code","colab":{}},"source":["def bayes_tester(testing_table:dframe, evidence_bag:dict, training_table:dframe, parser:Callable) -> list:\n","  assert isinstance(testing_table, pd.core.frame.DataFrame), f'test_table not a dataframe but instead a {type(testing_table)}'\n","  assert isinstance(evidence_bag, dict), f'evidence_bag not a dict but instead a {type(evidence_bag)}'\n","  assert isinstance(training_table, pd.core.frame.DataFrame), f'training_table not a dataframe but instead a {type(training_table)}'\n","  assert callable(parser), f'parser not a function but instead a {type(parser)}'\n","  assert 'label' in training_table, f'label column is not found in training_table'\n","  assert training_table.label.dtype == int, f\"label must be an int column (possibly wrangled); instead it has type({training_table.label.dtype})\"\n","  assert 'text' in testing_table, f'text column is not found in testing_table'\n","\n","  #your code here\n","  return [bayes(set(parser(row['text'])), evidence_bag, training_table) for index, row in testing_table.iterrows()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yijloSkswqPT","colab_type":"code","outputId":"66f6fa54-23ec-4344-93c7-5f6fe8455510","executionInfo":{"status":"ok","timestamp":1579746856014,"user_tz":480,"elapsed":20686,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["%%time\n","\n","prob_list = bayes_tester(bayes_testing_table, char_bag, training_table, char_set_builder)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 11.3 s, sys: 14.3 ms, total: 11.3 s\n","Wall time: 11.3 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yWtBrRNiyKtq","colab_type":"code","outputId":"b9a94946-d0c5-461b-f4a6-3aef556fa9c7","executionInfo":{"status":"ok","timestamp":1579746856014,"user_tz":480,"elapsed":20681,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["prob_list[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1.832109487333442e-05, 2.5591876996227335e-06),\n"," (0.15900835851937165, 0.014463929660598234),\n"," (0.022763656207971168, 0.003299626184482346),\n"," (0.008792432048100953, 0.001215500810699199),\n"," (0.013079046652190255, 0.001951518430251379),\n"," (0.02180312247975622, 0.002475393220562259),\n"," (0.0267059396671276, 0.00405406478822823),\n"," (0.13116754249938786, 0.010487708053576284),\n"," (0.0005928977307361215, 7.236560549700605e-05),\n"," (0.00041528213972383585, 0.0001015697314194241)]"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"HELM8rPI-KIc","colab_type":"text"},"source":["My results below. Note that it is not unusual for your values to be different at the miniscule level. Make sure they match up to 5 digits.\n","<pre>\n","[(1.832109487333443e-05, 2.5591876996227335e-06),\n"," (0.15900835851937167, 0.014463929660598234),\n"," (0.02276365620797117, 0.0032996261844823473),\n"," (0.008792432048100953, 0.001215500810699199),\n"," (0.013079046652190254, 0.0019515184302513792),\n"," (0.02180312247975622, 0.0024753932205622588),\n"," (0.026705939667127598, 0.00405406478822823),\n"," (0.13116754249938783, 0.010487708053576284),\n"," (0.0005928977307361218, 7.236560549700605e-05),\n"," (0.0004152821397238359, 0.00010156973141942408)]\n"," </pre>"]},{"cell_type":"markdown","metadata":{"id":"-6oFd_-mySPN","colab_type":"text"},"source":["# Challenge 6\n","\n","Go ahead and compute the confusion dictionary that has the 4 cases."]},{"cell_type":"code","metadata":{"id":"ZA70vS1JybUr","colab_type":"code","colab":{}},"source":["actuals = testing_table.label.to_list()  #you need the actual labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yt99J3fNyg0_","colab_type":"code","colab":{}},"source":["#your code here\n","\n","case_dict = {(0, 0): 0, (0, 1): 0, (1, 0): 0, (1, 1): 0}\n","index = 0\n","for i in prob_list:\n","  node = (0 if i[0] > i[1] else 1, actuals[index])\n","  for i in case_dict:\n","    if node == i:\n","      case_dict[i] += 1\n","  index += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5RDrSJNGz1vd","colab_type":"code","outputId":"2bf69620-3000-42f9-c542-41b56c389232","executionInfo":{"status":"ok","timestamp":1579746856016,"user_tz":480,"elapsed":20667,"user":{"displayName":"Future Deus","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDA38nWUBTrQ7no755EmiqFEck1JlL77eTTYOhS=s64","userId":"11509427916790811607"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["case_dict  #{(0, 0): 9172, (0, 1): 664, (1, 0): 0, (1, 1): 0}"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{(0, 0): 9172, (0, 1): 664, (1, 0): 0, (1, 1): 0}"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"QQra7Xfa0afo","colab_type":"text"},"source":["##Epic fail!\n","\n","I can just look at the values and see that I am always predicting 0. And if I look at the first 10 values in prob_list, this is confirmed. The 0th item is always bigger than the 1st item. Sometimes not by much. But bigger all the same.\n","\n","My conjecture is that most of the tweets contain similar chars. So chars are not a good way to differentiate. So what is really making the difference is the P(O), i.e., P(hate) and P(nohate). Reminder: P(nohate) is 0.9316321862451075. I conjecture that if the P(O) probabilities were closer, we would get better results.\n","\n","But seriously, no one uses chars as evidence. It is words that we want. We are set up for that. All we have to do is define a new parser. That is coming up next week :)"]},{"cell_type":"markdown","metadata":{"id":"Zicx-qf4-0Y1","colab_type":"text"},"source":["##Reminder to update your library with our new functions\n","\n"]}]}